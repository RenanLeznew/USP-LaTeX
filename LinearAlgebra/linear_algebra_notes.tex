\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{amsfonts}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{fancyhdr}
\usepackage[portuguese]{babel}
\usepackage{hyperref}
\usepackage{lastpage}
\usepackage{mathtools}

\pagestyle{fancy}
\fancyhf{}

\pgfplotsset{compat = 1.18}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\newtheorem*{def*}{\underline{Defini\c c\~ao}}
\newtheorem*{theorem*}{\underline{Teorema:}}
\newtheorem{example}{\underline{Exemplo:}}[section]
\newtheorem*{proposition*}{\underline{Proposi\c c\~ao:}}
\newtheorem*{corol*}{\underline{Corol\'ario:}}
\newtheorem*{sol*}{\underline{Solu\c c\~ao:}}
\newtheorem*{proof*}{\underline{Prova:}}
\renewcommand\qedsymbol{$\blacksquare$}
\newcommand{\Lin}[1]{Lin_{\mathbb{K}}({#1})}

\rfoot{P\'agina \thepage \hspace{1pt} de \pageref{LastPage}}

\title{EXERC\'ICIOS DE C\'ALCULO}
\author{Renan Wenzel}
\date{\today}

\begin{document}

\begin{figure}[ht]
	\minipage{0.76\textwidth}
	\includegraphics[width=4cm]{icmc.png}
	\hspace{5cm}
	\includegraphics[height=4.9cm,width=4cm]{brasao_usp_cor.jpg}
	\endminipage
\end{figure}

\begin{center}
	\vspace{1cm}
	\LARGE
	UNIVERSIDADE DE S\~AO PAULO

	\vspace{1.3cm}
	\LARGE
	INSTITUTO DE CI\^ENCIAS MATEM\'ATICAS E COMPUTACIONAIS - ICMC

	\vspace{1.7cm}
	\Large
	\textbf{ANOTA\c C\~OES E RESUMOS DE \'ALGEBRA LINEAR}

	\vspace{1.3cm}
	\large
	\textbf{Renan Wenzel - 11169472}

	\vspace{1.3cm}
	\today
\end{center}

\newpage

\tableofcontents

\newpage

\section{Introdu\c c\~ao}

A \'algebra linear se preocupa em generalizar alguns conceitos fundamentais da matem\'atica.
Durante o ensino m\'edio e no primeiro semestre, o estudante aprendeu sobre a estrutura
dos n\'umeros inteiros, reais, racionais e, talvez, sobre os complexos. Agora, vamos ver o que
tem de mais profundo por tr\'as deles, estudando, por exemplo, quest\~oes da dimens\~ao, de
opera\c c\~oes e dos objetos que sofrem delas.

Na segunda parte do curso, o estudante costuma aprender sobre funcionais e transforma\c c\~oes
lineares, que relacionam dois espa\c cos vetoriais (ou um corpo) e satisfazem algumas condi\c c\~oes.
Um exemplo j\'a visto de uma transforma\c c\~ao linear \'e a derivada, que serve de motiva\c c\~ao
para uma constru\c c\~ao futura chamada Espa\c co Tangente.

Saber manipular e trabalhar com esses conceitos \'e essencial na carreira de matem\'atico, porque
muitos conceitos futuros tomam como base o que ser\'a visto aqui, em todas as \'areas da matem\'atica e,
para isso, espero que essas notas sirvam como um guia para quem necessita de uma luz nesse curso.


\section{Espa\c cos Vetoriais}
Para falar sobre espa\c cos vetoriais, \'e preciso antes falar sobre corpos, pois espa\c cos
vetoriais s\~ao definidos com base nessas estruturas, que ser\~ao ``onde os vetores ir\~ao morar".
Para exemplificar, um espa\c co vetorial sobre $\mathbb{R}$ ter\'a como vetores n\'umeros reais,
e um espa\c co vetorial sobre $\mathbb{C}$ ter\'a como vetores n\'umeros complexos. O que
faremos a seguir \'e definir mais precisamente cada um destes conceitos, desde corpos a vetores.
\subsection{Corpos e Espa\c cos Vetoriais}
\begin{def*}
	\text{Diremos que um conjunto} $\mathbb{K}$ \text{\'e  um corpo se ele satisfaz as seguintes
		propriedades: }
	\begin{itemize}
		\item [A1)] x + y = y + x, $\forall x, y\in\mathbb{K}$
		\item [A2)] (x + y) + z = x + (y + z), $\forall x, y, z\in\mathbb{K}$
		\item [A3)] \text{Existe um elemento 0 tal que } x + 0 = 0 + x = x, $\forall x \in\mathbb{K}$
		\item [A4)] \text{Para todo x, existe um elemento -x tal que } x + (-x) = -x + x = 0, $\forall x \in\mathbb{K}$.
		\item [M1)] $x\cdot{}y = y\cdot{}x, \forall x, y\in\mathbb{K}$
		\item [M2)] ($x\cdot{y})\cdot{z} = x(y\cdot{z}), \forall x, y, z \in\mathbb{K}$
		\item [M3)] \text{Existe um elemento 1 tal que } $x\cdot{1} = 1\cdot{x} = x, \forall x \in\mathbb{K}$
		\item [M4)] \text{Para todo x, existe um elemento } $x^{-1}$ \text{ tal que } $x^{-1}\cdot{}x = x\cdot{x^{-1}} = 1, \forall x \in\mathbb{K}$
		\item [D1)] $x\cdot{}(y + z) = x\cdot{y} + x\cdot{z}$
		\item [D2)] $(x + y)\cdot{}z = x\cdot{z} + y\cdot{z}$
	\end{itemize}
\end{def*}
Faremos algumas conven\c c\~oes com rela\c c\~ao a essa defini\c c\~ao. Ao inv\'es de escrevermos x + (-x), $x\cdot{y}, x^{-1}$, faremos:
$$
	x - x := x + (-x), \quad x\cdot{y} := xy, \quad x^{-1} := \frac{1}{x}.
$$
Exemplos b\'asicos de corpos s\~ao o corpo dos n\'umeros reais, dos n\'umeros racoinais e dos n\'umeros complexos.
Um bom exerc\'icio para se acostumar com esses conceitos \'e provar que eles s\~ao realmente corpos e mostrar que
os n\'umeros inteiros n\~ao formam um corpo.

Agora que temos uma ideia sobre corpos, podemos definir um espa\c co vetorial:
\begin{def*}
	Dizemos que um conjunto V \'e um espa\c co vetorial se seus elementos, chamados vetores, satisfazem os axiomas abaixo:
	\begin{itemize}
		\item [V1)] u + v = v + u, $v, u\in{V}$; \label{V1}
		\item [V2)] u + (v + w) = (u + v) + w, $v, u, w\in{V}$; \label{V2}
		\item [V3)] Existe $0\in{V}$ tal que v + 0 = v, $v\in{V}$; \label{V3}
		\item [V4)] Para todo $v\in{V}, $ existe $-v\in{V}$ tal que v - v = 0; \label{V4}
		\item [E1)] Dado $v\in{V}, 1v = v$; \label{E1}
		\item [E2)] Dados $\alpha, \beta\in\mathbb{K}, (\alpha\beta)v = \alpha(\beta{v}), v\in{V}$; \label{E2}
		\item [DV1)] $\alpha(u + v) = \alpha{u} + \alpha{v}$; \label{DV1}
		\item [DV2)] $(\alpha + \beta)v = \alpha{v} + \beta{v}$. \label{DV2}
	\end{itemize}
\end{def*}

Vamos ver um exemplo de espa\c co vetorial:

\begin{example}
	\label{Vspace}
	Considere o corpo $\mathbb{R}^2$. Vamos mostrar que $\mathbb{R}$ \'e um espa\c co vetorial sobre $\mathbb{R}^2$.
	Esse tipo de demonstra\c c\~ao \'e, em geral, sempre igual. Tome dois elementos $v, u\in\mathbb{R}$ e dois escalares
	$\mathbf{k_1}, \mathbf{k_2}\in\mathbb{R}^2$.

	Segue das propriedades dos n\'umeros reais que existe um elemento neutro da adi\c c\~ao, o 0 usual, um inverso
	aditivo (dado $x\in\mathbb{R}, -x\in\mathbb{R}$) e as propriedades usuais de soma, isto \'e, comutatividade e
	associatividade. Agora, coloque $\mathbf{k_1} = (\alpha_1, \beta_1), \mathbf{k_2 = (\alpha_2, \beta_2)}$ e
	1 = (1, 0).
	Com isto, temos:
	$$
		1.x = (1, 0).x = (1.x, 0.x) = (x, 0) = x,
	$$
	$$
		(\mathbf{k_1}\mathbf{k_2})x = ((\alpha_1\alpha_2, \beta_1\beta_2))(x, 0) = ((\alpha_1\alpha_2){x}, \beta_1\beta_2{0}) =
		= (\alpha_1(\alpha_2{x}), 0) = (\alpha_1, \beta_1)((\alpha_2, \beta_2)(x, 0)) = k_1(k_2x).
	$$
	Resta mostrar a distributiva. Note que
	$$
		(\mathbf{k_1} + \mathbf{k_2})x = (\alpha_1 + \alpha_2, \beta_1 + \beta_2)x = (\alpha_1x + \alpha_2x, 0) = (\alpha_1, \beta_1)x + (\alpha_2, \beta_2)x.
	$$
	e
	$$
		(x + y)\mathbf{k_1} = ((x, 0) + (y, 0))(\alpha_1, \beta_1) = (x + y, 0)(\alpha_1, \beta_1) = (x\alpha_1 + y\alpha_1, 0) = x\mathbf{k_1} + y\mathbf{k_1}.
	$$
	\qedsymbol
\end{example}
Agora que temos uma no\c c\~ao b\'asica de espa\c cos vetoriais, aprofundaremos na teoria.

\subsection{Bases}
Sabe como todo n\'umero real pode ser escrito como 1.x? Vamos buscar uma forma an\'aloga para um espa\c co vetorial
qualquer. Para isso, ser\'a introduzido o conceito da combina\c c\~ao linear e independ\^encia linear de vetores quaisquer.

\begin{def*}
	Dado um  espa\c co vetorial V sobre $\mathbb{K}$, diremos que um conjunto de vetores $\mathcal{B}: v_1, \cdots{}, v_n$ gera V
	se qualquer elemento $v\in{V}$ puder ser escrito como:
	$$
		v = \sum_{i=1}^{n}\alpha_iv_i,
	$$
	com $\alpha_i\in\mathbb{K}$ para cada i. Dado um conjunto $\mathcal{B}$, dizemos que ele \'e um conjunto
	gerador de V se todo elemento de V pode ser escrito como combina\c c\~ao linear finita de elementos de
	$\mathcal{B}$.
\end{def*}
H\'a, por\'em, um problema com isso. Vamos ilustrar isso no exemplo a seguir:
\begin{example}[NB]
	\label{Not basis}
	Considerando $\mathbb{C}^2$ como um espa\c co vetorial sobre $\mathbb{C}$, o conjunto
	$\mathcal{B}: (1, 0), (0, i), (i, 0), (0, 1)$ gera $\mathbb{C}^2$.

	De fato, visto que um elemento $(a, b) = (x + iy, z + iw)$, em que $a, b\in\mathbb{C}, x, y, z, w\in\mathbb{R}$, pode ser escrito como:
	$$
		(a, b) = x(1, 0) + y(i, 0) + z(0, 1) + w(0, i)
	$$
	No entanto, observe que se $(a, b) = (0, 0)$, ent\~ao
	$$
		(0, 0) = 1(1, 0) + i(i, 0) + 0(0, 1) + 0(0, i).
	$$
	Assim, o elemento $(0, 0)$ pode ser escrito de duas formas diferentes! Sendo a outra:
	$$
		(0, 0) = 0(1, 0) + 0(i, 0) + 0(0, 1) + 0(0, i).
	$$
\end{example}
Explicitamente, queremos representar \textbf{de maneira \'unica} cada elemento de V. \'E para isso que surge
a no\c c\~ao de independ\^encia linear e de base, isto \'e,
\begin{def*}
	Dado um  espa\c co vetorial V sobre $\mathbb{K}$, diremos que um conjunto de vetores $\mathcal{B}: v_1, \cdots{}, v_n$ gera V
	se:
	$$
		\sum_{i=1}^{n}\alpha_iv_i = 0 \Leftrightarrow \alpha_1 = \alpha_2 = \cdots = \alpha_n = 0,
	$$
	com $\alpha_i\in\mathbb{K}$ para cada i. Caso isso n\~ao ocorra, ou seja, existe ao menos um $\alpha_i\in\mathbb{K}\neq{0}$ para cada i.
	tal que 0 pode ser escrito como combina\c c\~ao linear que inclua $\alpha_iv_i$, diremos que o conjunto \'e
	linearmente dependente.
\end{def*}

\begin{def*}
	Dado um  espa\c co vetorial V sobre $\mathbb{K}$, diremos que um conjunto de vetores $\mathcal{B}: b_1, b_2, \cdots{}, b_n$ \'e uma base de V
	$\mathcal{B}$ gera V e \'e linearmente independente.
\end{def*}

Conclui-se que o conjunto apresentado no exemplo \ref{Not basis} n\~ao \'e uma base! (por que?). Com base nisso,
voc\^e consegue encontrar uma base para o espa\c co vetorial do exemplo? E para o exemplo \ref{Vspace}?

A seguir, seguem algumas propriedades dos conceitos vistos acima que s\~ao \'uteis para treinar demonstra\c c\~ao,
ent\~ao recomendo que voc\^es tentem:

\begin{proposition*}
	\begin{itemize}
		\item[a)] Seja $\mathcal{B}$ um conjunto gerador de V. Ent\~ao, todo subconjunto W de V que cont\'em $\mathcal{B}$ \'e um
		      conjunto gerador de V.
		\item[b)] Todo conjunto contendo o vetor nulo \'e linearmente dependente.
		\item[c)] Todo subconjunto de um conjunto linearmente independente \'e linearmente independente.
	\end{itemize}
\end{proposition*}

Seguem abaixo alguns resultados um pouco mais complicados para serem deixados como exerc\'icio, junto de uma nova defini\c c\~ao.

\begin{def*}
	Dizemos que um espa\c co vetorial V sobre $\mathbb{K}$ \'e finitamente gerado se possuir um conjunto gerador finito.
\end{def*}

\begin{proposition*}
	Seja V um $\mathbb{K}$-espa\c co vetorial finitamente gerado n\~ao nulo e suponha que $\{v_1, \cdots, v_m\}$ seja
	um conjunto gerador de V. Ent\~ao todo conjunto linearmente independente de vetores em V tem no m\'aximo m elementos.
\end{proposition*}
\begin{proof*}
	Vamos mostrar algo equivalente a isso, ou seja, se um conunto tem mais que m elementos, ele \'e linearmente dependente.
	Com efeito, considere $\mathcal{A} = \{v_1, \cdots, v_n\} \subset{V}, n > m.$ Como $\{v_1, \cdots, v_m\}$ gera V, associamos
	para cada \'indice $u_j, j = 1, \cdots, n$ uma combina\c c\~ao linear da forma:
	$$
		u_{j} = \alpha_{1j}v_1 + \cdots + \alpha_{mj}v_m = \sum_{i=1}^{m} \alpha_{ij}v_i.
	$$
	Agora, vamos supor que o conjunto dos $u_{j}'s$ gera 0, ou seja,
	$$
		0 = \sum_{j=1}^{n} \lambda_{j}u_{j}.
	$$
	Reescrevendo cada $u_{j}$, temos
	$$
		0 = \sum_{j=1}^{n} \lambda_{j}u_{j} = \sum_{j=1}^{n}\lambda_{j}\underbrace{\sum_{i=1}^{m}{\alpha_{ij}v_i}}_{\mathclap{\substack{\text{essa soma vem da}, \\ \text{expans\~ao dos } u_j, \\ \text{sei que pode parecer intimidador,} \\ \text{mas juro que n\~ao morde!}.}}}
	$$
	$$
		= \underbrace{\sum_{j=1}^{n}\sum_{i=1}^{m}}_{\mathclap{\substack{\text{reorganizei a soma} \\ \text{pra ficar mais limpo visualmente}}}} \lambda_{j}\alpha_{ij}v_{i} \overbrace{=}^{\text{inverte a ordem}} \sum_{i=1}^{m}\biggl(\sum_{j=1}^{n}\lambda_{j}\alpha_{ij}\biggr)v_{i}
	$$

	Agora que temos essa soma, pense em cada termo de $\sum_{j=1}^{n}\lambda_{j}\alpha_{ij}$ como um escalar (a soma de escalares \'e um escalar) e suponha que
	$\sum_{j=1}^{n}\lambda_{j}\alpha_{ij} = 0$ para analisarmos o tipo de depend\^encia desse termo. Para cada $i = 1, 2, \cdots, m$, temos
	uma equa\c c\~ao do tipo
	$$
		\lambda_{1}\alpha_{i1} + \lambda_{2}\alpha_{i2} + \cdots + \lambda_{n}\alpha_{in} = 0,
	$$
	o que sugere um sistema linear para cada valor de i na inc\'ognita $\lambda_{j}$:
	$$
		\left\{\begin{array}{ll}
			\lambda_{1}\alpha_{11} + \lambda_{2}\alpha_{12} + \cdots + \lambda_{n}\alpha_{1n} = 0, \\
			\lambda_{1}\alpha_{21} + \lambda_{2}\alpha_{22} + \cdots + \lambda_{n}\alpha_{2n} = 0, \\
			\vdots                                                                                 \\
			\lambda_{1}\alpha_{m1} + \lambda_{2}\alpha_{m2} + \cdots + \lambda_{n}\alpha_{mn} = 0,
		\end{array}\right.
	$$
	Agora, como n \'e maior que m, h\'a mais inc\'ognitas que equa\c c\~oes! Em outras palavras, o sistema possui ao menos
	uma solu\c c\~ao n\~ao nula, ou seja, existem $\gamma_1, \cdots, \gamma_n\in\mathbb{K}$ tais que nem todos s\~ao nulos e
	$$
		\sum_{j=1}^{n} \gamma_{i}\alpha_{ij} = 0.
	$$
	Disto segue que existem $\gamma_{i}'s$ nem todos nulos tais que
	$$
		0 = \sum_{i=1}^{m}\biggl(\sum_{j=1}^{n}\gamma_{j}\alpha_{ij}\biggr)v_{i} = \sum_{j=1}^{n}\sum_{i=1}^{m}\lambda_{j}\alpha_{ij}v_{i} =
	$$
	$$
		= \sum_{j=1}^{n}\sum_{i=1}^{m}\lambda_{j}u_{j}.
	$$
	Portanto, $\{u_1, \cdots, u_n\}$ \'e linearmente dependente, consequentemente $\mathcal{A}$ \'e tamb\'em.
\end{proof*}
\begin{corol*}
	Seja V um $\mathbb{K}$-espa\c co vetorial finitamente gerado n\~ao nulo. Ent\~ao, duas bases quaisquer de V t\^em
	o mesmo n\'umero de elementos.
\end{corol*}

Em vista disso, podemos definir o tamanho de um espa\c co vetorial unicamente de acordo com o n\'umero de elementos
da base, o que chamamos de dimens\~ao:

\begin{def*}
	Seja V um $\mathbb{K}$-espa\c co vetorial. Se V admite uma base finita, chamamos de dimens\~ao de V o n\'umero
	de elementos da base. Em outras palavras, se a base tem n elementos, ent\~ao
	$$
		\dim_{\mathbb{K}}V = n.
	$$
	Caso n\~ao exista base finita, dizemos que V possui dimens\~ao infinita.
\end{def*}

Alguns resultados que eu n\~ao vou provar por um tempo seguem:
\begin{proposition*}
	Seja V um $\mathbb{K}$-espa\c co vetorial. Se V possui dimens\~ao $dim_{\mathbb{K}}V = n \geq 1$ e seja
	$\mathcal{B}$ um subconjunto de V com n elementos. As seguintes afirma\c c\~oes s\~ao equivalentes:
	\begin{itemize}
		\item [a)] $\mathcal{B}$ \'e uma base.
		\item [b)] $\mathcal{B}$ \'e linearmente independente.
		\item [c)] $\mathcal{B}$ \'e um conjunto gerador.
	\end{itemize}
\end{proposition*}
\begin{proposition*}
	\label{Base_completion}
	Seja V um $\mathbb{K}$-espa\c co vetorial e considere $\mathcal{B} = \{v_1, \cdots, v_m\}$ um conjunto linearmente
	independente em V. Se existir $v\in{V}$ que n\~ao seja combina\c c\~ao linear dos elementos de $\mathcal{B}$, ent\~ao
	$\{v_1, \cdots, v_m, v\}$ \'e  linearmente independente. (Isso permite o m\'etodo de completamento de base!).
\end{proposition*}
\begin{theorem*}
	Todo espa\c co vetorial finitamente gerado n\~ao nulo possui uma base.
\end{theorem*}
\begin{proposition*}
	Seja V um $\mathbb{K}$-espa\c co vetorial de dimens\~ao $n\geq{1}$ e seja $\mathcal{B}\subseteq{V}$. As seguintes
	afirma\c c\~oes s\~ao equivalentes:
	\begin{enumerate}
		\item [a)] $\mathcal{B}$ \'e uma base de V.
		\item [b)] Cada elemento de V se escreve unicamente como combina\c c\~ao linear de elementos de $\mathcal{B}.$
	\end{enumerate}
\end{proposition*}

Como conseque\^encia dessa \'ultima proposi\c c\~ao, podemos introduzir a noc\c c\~ao de coordenada. Seja V
um $\mathbb{K}$-espa\c co vetorial de dimens\~ao $n \geq{1}$ e seja $\mathcal{B} = \{v_1, \cdots, v_n\}$ uma base.
Vamos fixar a ordem dos elementos de $\mathcal{B}$, formando o que chamamos de base ordenada de V, e agora aplicamos
a proposi\c c\~ao acima.

Dado $v\in{V}$, existem \'unicos $\alpha_1, \cdots, \alpha_n\in\mathbb{K}$ tais que $v = \sum_{i=1}^{n}\alpha_iv_i$.
Por conta dessa unicidade, \'e comum determinarmos v puramente pelos coeficientes, e escrevemos
$$
	[v] = (\alpha_1, \alpha_2, \cdots, \alpha_n)_{\mathcal{B}}
$$
e dizemos que $\alpha_1, \alpha_2, \cdots, \alpha_n$ s\~ao as coordenadas de v com rela\c c\~ao \`a base $\mathcal{B}.$

\subsection{Subespa\c cos Vetoriais e Somas de Espa\c cos}
Tal como um conjunto possui um subconjunto, sob quais condi\c c\~oes um espa\c co vetorial possui
outro espa\c co vetorial ``dentro de si"? Esta no\c c\~ao, conhecida como subespa\c co vetorial,
inclui conceitos como $\mathbb{R}^2$ ser basicamente ``duas retas $\mathbb{R}$", sobre todo n\'umero
racional ser tamb\'em um n\'umero real e sobre todo n\'umero real ser um n\'umero complexo.

Com base nessa motiva\c c\~ao, damos continuidade para formalizar esses conceitos.

\begin{def*}
	Seja V um espa\c co vetorial sobre $\mathbb{K}$ e W um subconjunto \textbf{do conjunto V}.
	Diremos que W \'e um subespa\c co vetorial de V se:
	\begin{itemize}
		\item [SS1)] $0\in{W}$ \label{SS1}
		\item [SS2)] Se $w_1, w_2\in{W}$, ent\~ao $w_1 + w_2 \in{W}$ \label{SS2}
		\item [SS3)] Dados $w_1 \in{W}, k\in\mathbb{K}$, ent\~ao $kw_1 \in{W}.$ \label{SS3}
	\end{itemize}
\end{def*}

Vamos checar que W com essas condi\c c\~oes \'e realmente um espa\c co vetorial:

Sejam $w_1, w_2, w_3\in{W}$ e $k_1, k_2\in\mathbb{K}$. Como $0\in{W}$, segue da propriedade \ref{SS2}
que $w_1 + 0 = w_1 \in{W}$, satisfazendo a propriedade \ref{V4} de espa\c co vetorial. Al\'em disso, sendo
em particular $w_1, w_2, w_3\in{V}$ (pois $W\subseteq{V}$), eles satisfazem \ref{V1}, \ref{V2} e \ref{V3}.
Juntando isso com a propriedade \ref{SS2}, segue que $w_1 + w_2 = w_2 + w_1, w_1 + (w_2 + w_3) = (w_1 + w_2) + w_3$ e
existe $-w_1\in{W}$ tal que $w_1 - w_1 = 0$ e todas essas opera\c c\~oes est\~ao em W por conta de \ref{SS2}.
\paragraph*{} Agora, vamos verificar as opera\c c\~oes com escalares. Como 1 \'e um escalar, segue
da propriedade \ref{E1} que $1w_1 = w_1$ e, por conta de \ref{SS3}, $1.w_1\in{W}.$ Al\'em disso,
de \ref{E2} e \ref{SS3}. \'e v\'alido em W que $(k_1k_2)w_1 = k_1(k_2w_1)\in{W}.$ Logo, W
satisfaz as opera\c c\~oes dos escalares.
\paragraph*{} Por fim, a checagem da distributiva \'e como feito acima, usando as opera\c c\~oes
de V como espa\c co vetorial e \ref{SS2}, \ref{SS3}.

Portanto, W \'e um espa\c co vetorial dentro de V! Um exerc\'icio bom para treinar a ideia \'e
tentar provar que $\mathbb{Q}$ \'e subespa\c co de $\mathbb{R}$.

Se tivermos dois espa\c cos vetoriais $V_1, V_2$, uma constru\c c\~ao importante \'e a soma
deles como espa\c cos. Assim, obtemos um espa\c co $V_1 + V_2$ sobre $\mathbb{K}$ cujos elementos
s\~ao da forma $v_1 + v_2, v_1\in{V_1}, v_2\in{V_2}$. (Fica de exerc\'icio conferir provar
que \'e de fato um espa\c co vetorial). Um tipo espec\'ifico de soma de espa\c cos importante
\'e aquele em que cada elemento pode ser escrito de maneira \'unica como soma de elementos de cada
um, formando o que se chama Soma Direta:
\begin{def*}
	Dados dois espa\c cos vetoriais $V_1, V_2$ tais que $V_1\cap{V_2} = \{0\}$, escrevemos
	$$
		V_1 \oplus{V_2}
	$$
	para denotar a soma desses espa\c cos, e chamamos ela de Soma Direta.
\end{def*}

\begin{example}
	Considere a reta real $\mathbb{R}$ e o conjunto dos n\'umeros imagin\'arios $\mathbb{I}.$ Note
	que o \'unico n\'umero que \'e tanto um real quanto um imagin\'ario \'e o 0, pois $0.i = 0$.
	Em outras palavras, $\mathbb{R}\cap\mathbb{I} = \{0\}.$

	Agora, \'e poss\'ivel definir um espa\c co vetorial que seja a soma direta de ambos e que cada elemento ter\'a a forma
	$a + b, a\in\mathbb{R}, b\in\mathbb{I}$. No entanto, como $b\in{\mathbb{I}}, b = ix, x\in\mathbb{R}
		, i = \sqrt{-1}.$ Logo, temos um elemento da forma $a + ix \in\mathbb{R}\oplus\mathbb{I}.$
	Isso te lembra alguma coisa? Caso n\~ao, note que esse n\'umer obtido \'e um n\'umero complexo,
	ou seja, a soma direta de $\mathbb{R}$ com $\mathbb{I}$ \'e:
	$$
		\mathbb{R}\oplus\mathbb{I} := \mathbb{C}.
	$$

	\textbf{Observa\c c\~ao:} o conjunto dos imagin\'arios $\mathbb{I}$ \'e o dos n\'umeros
	que possuem a forma $\mathbb{I}=\{ix: x\in\mathbb{R}, i = \sqrt{-1}\}.$
\end{example}

\begin{proposition*}
	Sejam V um espa\c co vetorial e $W_1, W_2$ dois subespa\c cos vetoriais de V, ambos de dimens\~ao finita. Ent\~ao,
	$$
		\dim_{\mathbb{K}} (W_1 + W_2) = \dim_{\mathbb{K}} W_1 + \dim_{\mathbb{K}} W_2 + \dim_{\mathbb{K}}(W_1\cap W_2).
	$$
\end{proposition*}
\begin{proof*}
	Essa demonstra\c c\~ao serve como um grande exemplo de manipula\c c\~ao de bases, isto \'e, completamento e remo\c c\~ao
	de elementos da base. Vamos supor, a priori, que a intersec\c c\~ao dos dois espa\c cos \'e n\~ao-trivial, ou seja,
	$W_1\cap{W_2} \neq \{0\}$ e seja $\mathcal{B} = \{w_1, \cdots, w_n\}$ uma base de $W_1\cap{W_2}$. Como $W_1\cap{W_2}$
	\'e subespa\c co tanto de $W_1$ quanto de $W_2$, \'e poss\'ivel estender a base at\'e ambos.

	Considere $\mathcal{B'} = \{w_1, \dots, w_n, v_1, \cdots, v_r\}, \mathcal{B``} = \{w_1, \cdots, w_n, u_1, \cdots, u_s\}$
	bases de $W_1$ e de $W_2$ respectivamente. A constru\c c\~ao dessas duas bases tem como base ``aumentar o tamanho de
	$\mathcal{B}$ adicionando novos elementos". O objetivo disso \'e mostrar que o conjunto $\mathcal{C} = \{w_1, \cdots,
		w_n, v_1, \cdots, v_r, u_1, \cdots, u_s\}$ \'e uma base de $W_1 + W_2.$

	H\'a muitos \'indices aqui, ent\~ao recapitulando: come\c camos com uma base com n elementos da interse\c c\~ao $W_1\cap{W_2}$,
	representando a dimens\~ao de $W_1\cap{W_2}$, $\dim_{\mathbb{K}}(W_1\cap{W_2}) = n.$ A partir disso, adicionamos novos r
	elementos, representando a dimens\~ao de $W_1$, $\quad\dim_{\mathbb{K}}(w_1) = n + r$ e outros s elementos, representado
	a dimens\~ao de $W_2$, $\dim_{\mathbb{K}}(W_2) = n + s.$ O resultado que queremos provar se resume a encontrar uma base de
	$W_1 + W_2$ que possua
	$$
		\dim_{\mathbb{K}}(W_1) + \dim_{\mathbb{K}}(W_2) - \dim_{\mathbb{K}}(W_1\cap{W_2}) = n + r + n + s - n = n + r + s
	$$
	elementos, e por isso o nosso objetivo \'e aquele!

	Dando continuidade, come\c camos mostrando que $\mathcal{C}$ gera $W_1 + W_2$. Seja $v\in{W_1+W_2}$, ou seja,
	$v = w_1 + w_2, w_1\in{W_1}, w_2\in{W_2}$. Usando as bases de cada respectivo espa\c co e da interse\c c\~ao, temos
	$$
		w_1 = \sum_{i=1}^{n}\lambda_{i}w_{i} + \sum_{j=1}^{r}\gamma_{j}v_{j}
	$$
	e
	$$
		w_2 = \sum_{i=1}^{n}\alpha_{i}w_{i} + \sum_{l=1}^{s}\beta_{l}u_{l}.
	$$
	Assim, v pode ser escrito como
	$$
		v = w_1 + w_2 = \sum_{i=1}^{n}\lambda_{i}w_{i} + \sum_{j=1}^{r}\gamma_{j}v_{j} +
		\sum_{i=1}^{n}\alpha_{i}w_{i} + \sum_{l=1}^{s}\beta_{l}u_{l} =
	$$
	$$
		= \biggl(\sum_{i=1}^{n}\lambda_{i}w_{i} + \sum_{i=1}^{n}\alpha_{i}w_{i}\biggr) + \sum_{j=1}^{r}\gamma_{j}v_{j} + \sum_{l=1}^{s}\beta_{l}u_{l}
	$$
	$$
		= \sum_{i=1}^{n}(\lambda_{i} + \alpha_{i})w_{i} + \sum_{j=1}^{r}\gamma_{j}v_{j} + \sum_{l=1}^{s}\beta_{l}u_{l}.
	$$
	Portanto, como a soma $\lambda_{i} + \alpha_{i}$ \'e tamb\'em um escalar, v \'e combina\c c\~ao linear de elementos de $\mathcal{C}$ e,
	por termos escolhido v sendo qualquer, $\mathcal{C}$ gera $W_1 + W_2.$

	O pr\'oximo e \'ultimo passo \'e mostrar que $\mathcal{C}$ \'e linearmente independente. Considere a soma
	$$
		(2.3)\quad\sum_{i=1}^{n}\alpha_{i}w_{i} + \sum_{j=1}^{r}\gamma_{j}v_{j} + \sum_{l=1}^{s}\beta_{l}u_{l} = 0, \label{EQ1}
	$$
	em que $\alpha_{i}, \beta_{j}, \gamma_{l}\in\mathbb{K}$. Assim, reescrevendo a equa\c c\~ao, obtemos:
	$$
		\sum_{l=1}^{s}\beta_{l}u_{l} = \sum_{i=1}^{n}(-\alpha_{i})w_{i} + \sum_{j=1}^{r}(-\gamma_{j})v_{j}, \in{W_1\cap{W_2}}
	$$
	sendo, ao mesmo tempo, combina\c c\~ao linear de elementos de $\mathcal{B'}$ e de elementos de $\mathcal{B``}.$
	Logo, existem $\lambda_1, \cdots, \lambda_n\in\mathbb{K}$ tais que
	$$
		\sum_{l=1}^{s}\beta_{l}u_{l} = \sum_{i=1}^{n}\lambda_{i}u_{i},
	$$
	isto \'e,
	$$
		\sum_{l=1}^{s}\beta_{l}u_{l} + \sum_{i=1}^{n}(-\lambda_{i})u_{i} = 0.
	$$
	Como $\{u_1, \cdots, u_s, w_1, \cdots, w_n\}$ \'e linearmente independente, temos $\beta_l = 0$ para todo
	$l = 1, \cdots, s$ e $\lambda_i = 0$, para todo $i = 1, \cdots, n.$ Em particular, a equa\c c\~ao \ref{EQ1} se reduz
	a
	$$
		\sum_{i=1}^{n}\alpha_{i}w_{i} + \sum_{j=1}^{r}\gamma_{j}v_{j} = 0.
	$$
	Sendo $\{w_1, \cdots, w_n, v_1, \cdots, v_r\}$ \'e linearmente independente, teremos $\alpha_{i} = 0$ para todo
	$i = 1, \cdots, n$ e $\gamma_{j} = 0$ para todo $j = 1, \cdots, r$. Portanto, $\{w_1, \cdots, w_n, v_1, \cdots, v_r, u_1,
		\cdots, u_s\}$ \'e linearmente independente e uma base de $W_1 + W_2.$

	No caso em que $W_1\cap{W_2} = \{0\}$, sejam $\mathcal{B_1}, \mathcal{B_2}$ bases respectivas de $W_1, W_2$. Por um
	processo an\'alogo ao que foi feito, mostra-se que $\mathcal{B_1}\cup\mathcal{B_2}$ \'e uma base de $W_1 + W_2$.
	Portanto, a demonstra\c c\~ao est\'a completa.

\end{proof*}
Finalizamos esta parte com duas proposi\c c\~oes de exerc\'icios, j\'a que \'e uma aplica\c c\~ao do que foi visto
at\'e agora:
\begin{proposition*}
	Sejam V um $\mathbb{K}$-espa\c co vetorial e $W_1, W_2$ dois subespa\c cos de V. Ent\~ao, $V = W_1 \oplus W_2$ se e
	s\'o se cada elemento $v\in{V}$ se escreve de maneira \'unica como uma soma $x_1 + x_2$, com $x_1\in{W_1}, x_2\in{W_2}.$
\end{proposition*}
\begin{proposition*}
	Sejam V um $\mathbb{K}$-espa\c co vetorial finitamente gerado n\~ao nulo e $W_1$ um subespa\c co de V. Ent\~ao,
	existe um subespa\c co $W_2$ de V tal que $V = W_1 \oplus W_2$.
\end{proposition*}

\section{Transforma\c c\~oes Lineares}
Quando tratamos de conjuntos, qual \'e a forma usual de relacionar dois conjuntos A e B? Neste ponto, j\'a sabemos que a resposta para
isso s\~ao as fun\c c\~oes - ``Mapas`` que relacionam um elemento de A a um elemento de B. Mas, \`as vezes, n\~ao basta apenas relacion\'a-los de forma qualquer, no caso, buscamos propriedades bonitas para as fun\c c\~oes, e disso surgem os conceitos de injetividade,
sobrejetividade e bije\c c\~oes.

Queremos fazer a mesma coisa com espa\c cos vetoriais, mas prestando aten\c c\~ao em um detalhe extra: Precisamos
preservar a estrutura de espa\c co. Em outras palavras, que condi\c c\~oes queremos na fun\c c\~ao $T:U\rightarrow{V}$
entre os espa\c cos U e V que respeite a estrutura deles, e \'e para isso que surge a ideia de Transforma\c c\~ao
Linear.

\subsection{Defini\c c\~oes}

\begin{def*}
	Dados dois $\mathbb{K}-$espa\c cos U e V, uma fun\c c\~ao $T:U\rightarrow{V}$ \'e uma transforma\c c\~ao (ou aplica\c c\~ao) linear
	se
	\begin{itemize}
		\item [TL1)] $T(u_1 + u_2) = T(u_1) + T(u_2), \quad\forall u_1, u_2\in{U}$ \label{TL1}
		\item [TL2)] $T(\lambda u) = \lambda{T(u)}, \quad\forall\lambda\in{\mathbb{K}}, u\in{U}.$ \label{TL2}
	\end{itemize}
\end{def*}

Em particular, $T(\sum_{i=1}^{n}\lambda_iu_i) = \sum_{i=1}^{n}\lambda_iT(u_i).$
\begin{example}[Inj]
	Seja $U\leq{V}$ um subespa\c co de V e considere a aplica\c c\~ao de inclus\~ao $i:U\hookrightarrow V$ dada por
	i(u) = u, $u\in{U}$. Tome $u, u'\in{U}$ e $k\in\mathbb{K}$. Temos:
	$$
		i(u + u') = u + u' = i(u) + i(u')
	$$
	e
	$$
		i(ku) = ku = ki(u).
	$$
	Portanto, i \'e uma aplica\c c\~ao linear. N\~ao apenas isso, observe que, se $u\neq{u'},$ ent\~ao
	$$
		i(u) = u \neq u' = i(u'),
	$$
	em outras palavras, i \'e injetora!
	\qedsymbol
\end{example}
\begin{def*}
	Sejam U, V dois $\mathbb{K}$-espa\c cos vetoriais e $T: U\rightarrow V$ uma transforma\c c\~ao linear. Diremos
	que:
	\begin{itemize}
		\item [i)] T \'e monomorfismo se T for injetora; \label{MONO}
		\item [ii)] T \'e epimorfismo se T for sobrejetora; \label{EPIM}
		\item [iii)] T \'e isomorfismo se T for bijetora. \label{ISO}
	\end{itemize}
\end{def*}

\begin{proposition*}
	Suponha que U, V, W s\~ao $\mathbb{K}$-espa\c cos vetoriais e que $T: U\rightarrow V$, $S: V\rightarrow W$ s\~ao
	transforma\c c\~oes lineares. Ent\~ao, $S\circ{T}:U\rightarrow{W}$ \'e uma transforma\c c\~ao linear.
\end{proposition*}
\begin{proof*}
	Primeiramente, sejam $u_1, u_2, u\in{U}$ vetores e $k\in\mathbb{K}$ um escalar. Vamos provar as propriedades
	de transforma\c c\~ao linear de $S\circ{T}$, come\c cando pela soma. Por S e T serem transforma\c c\~oes lineares,
	temos:
	$$
		S\circ{T}(u_1 + u_2) = S(T(u_1 + u_2)) = S(T(u_1) + T(u_2)) = S(T(u_1)) + S(T(u_2)) = S\circ{T}(u_1) + S\circ{T}(u_2).
	$$
	Resta provar a propriedade de ``passar o escalar pra fora" e, novamente, utilizando o fato de S e T serem transforma\c c\~oes
	lineares, obtemos o seguinte:
	$$
		S\circ{T}(ku) = S(T(ku)) = S(kT(u)) = kS(T(u)) = k(S\circ{T})(u).
	$$
	Portanto, $S\circ{T}$ \'e uma transforma\c c\~ao linear.
	\qedsymbol
\end{proof*}
Denotamos o espa\c co de todas as transforma\c c\~oes lineares entre dois espa\c cos U e V por
$Lin_{\mathbb{K}}(U, V)$. Em outras palavras, o conjunto \'e dado por
$$
	Lin_{\mathbb{K}}(U, V):=\{T:U\rightarrow V: T \text{ \'e transforma\c c\~ao linear.}\}.
$$
Para transformar isso em um espa\c co propriamente dito, vamos definir as opera\c c\~oes nele:
\begin{itemize}
	\item $(T_1 + T_2)u := T_1u + T_2u$
	\item $(kT)u := kTu$.
\end{itemize}
O elemento neutro aqui \'e a transforma\c c\~ao $0: u\mapsto{0}$ (0(u) = 0 para todo $u\in{U}$), basta verificar
que (T+0)u = Tu para todo u de U.

Uma coisa importante que acontece \'e com rela\c c\~ao \`as bases de um espa\c co vetorial sob uma transforma\c c\~ao
linear - elas s\~ao preservadas. Especificamente,

\begin{proposition*}
	Sejam U e V $\mathbb{K}-$espa\c cos vetoriais e considere bases $b_1, \cdots, b_n$ de U e $v_1, \cdots, v_n$ de V.
	Ent\~ao, existe uma aplica\c c\~ao linear $A:U \rightarrow V$ \'unica e que satisfaz $Ab_j = v_j$ para todo
	$j = 1, \cdots, n$.
\end{proposition*}
\begin{proof*}
	Sabe-se que todo elemento $u\in{U}$ satisfaz a igualdade
	$$
		u = \sum_{i=1}^{n}\lambda_ib_i, \lambda_i\in\mathbb{K}.
	$$
	A partir disso, aplicando-se A, obtemos
	$$
		Au = A\biggl(\sum_{i=1}^{n}\lambda_ib_i = \sum_{i=1}^{n}\lambda_iAb_i,  \lambda_i\in\mathbb{K}\biggr).
	$$
	Com isso, vamos definir a aplica\c c\~ao desejada por $Au:= \sum_{i=1}^{n}\lambda_iv_i.$ Assim, segue que
	$Ab_i = v_i$ e segue a unicidade, pois temos:
	$$
		0 = Au - Au = \sum_{i=1}^{n}\lambda_iAb_i - \sum_{i=1}^{n}\lambda_iv_i = \sum_{i=1}^{n}\lambda_i(Ab_i - v_i).
	$$
	Resta checarmos que essa A \'e de fato uma transforma\c c\~ao linear. Para isso, tome outro elemento $u' =
		\sum_{i=1}^{n}\lambda_i'b_i.$ Vamos primeiro calcular A em u e, em seguida, em u', para assim compararmos a soma
	dos dois elementos. Com efeito,
	$$
		Au = \sum_{i=1}^{n}\lambda_iv_i \quad Au' = \sum_{i=1}^{n}\lambda_i'v_i
	$$
	de maneira que
	$$
		A(u+u') = \sum_{i=1}^{n}(\lambda_i + \lambda_i')v_i = \sum_{i}\lambda_iv_i + \sum_{i}^{n}\lambda_i'v_i
		= Au + Au'.
	$$
	Assim, resta provar apenas que A comuta com escalares. Nesta linha de racioc\'inio, dado um escalar $k\in\mathbb{K}$,
	percebemos que
	$$
		A(ku) = \sum_{i=1}^{n}k\lambda_iv_i = k\sum_{i=1}^{n}\lambda_iv_i = kAu.
	$$
	Portanto, A \'e uma aplica\c c\~ao linear \'unica que satisfaz a propriedade desejada.
	\qedsymbol
\end{proof*}
\begin{corol*}
	Se duas aplica\c c\~oes lineares coincidem em uma base linear, elas s\~ao iguais.
\end{corol*}
\begin{proof*}
	Sejam $A_1, A_2$ duas aplica\c c\~oes lineares coincidindo em uma base linear. Em outras palavras, dado
	um elemento da base $b\in{\mathcal{B}}$, vale que $A_1b = A_2b$. Assim, dado um elemento u de V, sabemos que
	$$
		u = \sum_{i=1}^{n}\lambda_ib_i,
	$$
	em que $\lambda_i\mathbb{K}$ s\~ao escalares e $b_i\in\mathcal{B}.$ Agora, nessas condi\c c\~oes,
	$$
		A_1u = \sum_{i=1}^{n}\lambda_iA_1b_i = \sum_{i=1}^{n}\lambda_iA_2b_i = A_2u.
	$$
	Portanto, as duas aplica\c c\~oes coincidem em todos os elementos, ou seja, s\~ao iguais.
	\qedsymbol
\end{proof*}
\begin{def*}
	Seja $A:U\rightarrow{V}$ uma aplica\c c\~ao entre $\mathbb{K}$-espa\c cos vetoriais. Chamamos $A^{-1}0 =
		\{u\in{U}: Au = 0\}$ de n\'ucleo (ou kernel) de A.
\end{def*}
Uma pergunta que surge \'e - j\'a que aplica\c c\~oes lineares preservam as opera\c c\~oes dos espa\c cos vetoriais,
ser\'a que elas preservam outras estruturas, como os subespa\c cos? A resposta, afinal, \'e sim!
\begin{proposition*}
	Seja $A:U\rightarrow{V}$ uma aplica\c c\~ao entre $\mathbb{K}-$espa\c cos vetoriais e sejam $U'\leq{U'}$ e
	$V'\leq{V}$ subespa\c cos. Ent\~ao, a imagem AU' e a imagem inversa $A^{-1}V'$ s\~ao subespa\c cos,
	$AU'\leq{V} \text{ e } A^{-1}V'\leq{U}.$ Em particular, o n\'ucleo de A \'e um subespa\c co de U.
\end{proposition*}
\begin{proof*}
	Nas hip\'oteses da proposi\c c\~ao, tome dois elementos $v_1, v_2\in{AU'}$ e $k\in\mathbb{K}$. Primeiramente,
	note que 0 = 0.Au = A(u.0) = A0, ou seja, $0\in{AU'}$. Al\'em disso,
	$$
		v_1 + v_2 = Au_1 + Au_2 = A(u_1 + u_2)
	$$
	e, como U' \'e um subespa\c co, $u_1 + u_2\in{U'}$. Assim, $v_1 + v_2\in{AU'}$. Al\'em disso,
	$$
		kv_1 =  kAu_1 = A(ku_1)
	$$
	novamente, como U' \'e um subespa\c co, $ku_1\in{U'}$ e $kv_1\in{AU'}$, mostrando que \'e um subespa\c co. Para
	$A^{-1}V'$, o processo \'e an\'alogo. Para o n\'ucleo, \'e claro que $0\in{A0}$, mas agora considere
	$w_1, w_2\in{A0}.$ Ent\~ao,
	$$
		A(w_1 + w_2) = A(w_1) + A(w_2) = 0 + 0 = 0,
	$$
	tal que $w_1 + w_2\in{A0}.$ Ademais, fornecido um escalar $k\in\mathbb{K}$,
	$$
		A(kw_1) = kAw_1 = k0 = 0,
	$$
	que tamb\'em partence a A0. Portanto, A0 \'e um subespa\c co vetorial.
	\qedsymbol
\end{proof*}
Uma vantajem de definirmos o n\'ucleo de uma aplica\c c\~ao linear \'e que ele nos fornece um crit\'erio para
a aplica\c c\~ao ser injetora:
\begin{proposition*}
	Seja $A:U\rightarrow{V}$ uma aplica\c c\~ao entre $\mathbb{K}-$espa\c cos vetoriais. Ent\~ao, A \'e um
	monomorfismo se, e s\'o se, o n\'ucleo de A \'e nulo.
\end{proposition*}
\begin{proof*}
	Por um lado, suponha que A \'e um monomorfismo (injetora). Ent\~ao, dado $u\in{U}$, Au = 0 implica que u = 0,
	isto \'e, $A0 = \{0\},$ provando o que quer\'iamos.

	Por outro lado, suponha que o n\'ucleo de A \'e nulo. Suponha que $u_1, u_2\in{U}$ s\~ao tais que $Au_1 = Au_2$,
	de forma que
	$$
		0 = Au_1 - Au_2 = A(u_1 - u_2).
	$$
	Em outras palavras, $u_1 - u_2\in{A0}$. Como o n\'ucleo \'e nulo, $u_1 - u_2 = 0$. Portanto, $u_1 = u_2$, e A
	\'e injetora.
\end{proof*}
\begin{proposition*}
	Seja $A:U\rightarrow{V}$ uma aplica\c c\~ao entre $\mathbb{K}$-espa\c cos vetoriais com U finitamente gerado e
	seja W um subespa\c co complementar ao n\'ucleo $N:= A^{-1}0$ de A, isto \'e, $U = N \oplus W$. Ent\~ao,
	$A|W: W \rightarrow AU$ \'e um isomorfismo.
\end{proposition*}
\begin{proof*}
	Considere um elemento $u\in{U}$ e seu respectivo $Au\in{AU}$. Tome tamb\'em um representante $v\in{N}$. Nessas
	condi\c c\~oes, observe que u pode ser escrito como u = w + v, para algum w de W, de modo que
	$$
		Au = A(w + v) = Aw + Av = Aw + 0 = Aw.
	$$
	Em outras palavras, $A|_W: W\rightarrow AU$ \'e sobrejetora, pois todo elemento de AU pode ser escrito como
	um esta aplica\c c\~ao agindo em um elemento de w. Al\'em disso, considere o n\'ucleo $N|_W:=\{w\in{W}: Aw = 0\}.$
	Ent\~ao, se $w\in{N|_W}$, temos Aw = 0, ou seja, $w\in{N}.$ Como $U = N\oplus{W}, N\cap{W} = \{0\}$ e, como
	$w\in{N\cap{W}}, w = 0.$ Logo, $N|_W = \{0\}$, donde segue que $A|_W$ \'e injetora. Portanto, $A|_W$ \'e um
	isomorfismo.
	\qedsymbol
\end{proof*}
Com este resultado em m\~aos, obtivemos uma caracteriza\c c\~ao importante: A imagem da Aplica\c c\~ao agindo
sobre um espa\c co vetorial U independe dos elementos do n\'ucleo, podendo ser vista como o resultado da a\c c\~ao
dela a uma restri\c c\~ao de seu dom\'inio. Outra consequ\^encia imediata de $U = N \oplus{W}$ \'e que
$\dim_{\mathbb{K}}A^{-1}0 + \dim_{\mathbb{K}} AU = \dim_{\mathbb{K}}$.
\begin{def*}
	A dimens\~ao da imagem de A \'e chamada posto de A, e ser\'a denotada por $rkA:=\dim_{\mathbb{K}}AU.$
\end{def*}

\subsection{Aplica\c c\~oes Lineares e Matrizes}

O pr\'oximo passo na jornada das aplica\c c\~oes lineares \'e uma liga\c c\~ao essencial entre elas e as matrizes.
Como ser\'a visto, toda matriz pode ser vista como uma aplica\c c\~ao linear e vice-versa. Essa rela\c c\~ao \'e,
de fato, muito mais intr\'inseca do que aparenta ser, n\~ao dependendo nem mesmo da base do espa\c co (por isso, diz
-se que \'e uma tradu\c c\~ao ``natural'' entre os dois conceitos). Vamos ver como construir isso.

Sejam U e V $\mathbb{K}$-espa\c cos vetoriais, seja $\beta: b_1, b_2, \cdots, b_n$ uma base linear de U e seja
$\gamma: c_1, c_2, \cdots, c_m$ uma base linear de V. Tomemos uma aplica\c c\~ao linear qualquer $A: U \rightarrow{V}$.
Ent\~ao, para todo $j = 1, 2, \cdots, n$, vale que $Ab_j = \sum\limits_{i=1}^{m}a_{ij}c_i$ para \'unicos $a_{ij}\in\mathbb{K}$.
Essa identifica\c c\~ao \'e respons\'avel por dar para cada elemento $b_j$ da base $\beta$ uma imagem escrita em termos
da base $\gamma$ com coordenada $(a_{1j}, a_{2j}, \cdots, a_{mj})$. Associamos \`a aplica\c c\~ao linear A a
($m \times n$)-matriz $[A]_{\gamma}^{\beta}:=[a_{ij}]$ (leia da seguinte forma: A representa a transfoma\c c\~ao dos elementos
da base $\beta$ \`a sua respectiva combina\c c\~ao linear em $\gamma$.) com coeficientes em $\mathbb{K}$. Com este processo,
associamos a cada transforma\c c\~ao linear uma matriz correspondente; agora, faremos a rec\'iproca disso.

Considere $[a_{ij}]$ uma ($m\times{n}$)-matriz qualquer com coeficientes em $\mathbb{K}$. Vimos que existe uma
\'unica aplica\c c\~ao linear $A:U\rightarrow{V}$ tal que $Ab_j = \sum\limits_{i=1}^{m}a_{ij}c_i$ para todo $j = 1,
	2, \cdots, n$. Com isso, quando bases s\~ao dadas em ambos os espa\c cos, temos um dicion\'ario entre as aplica\c c\~oes
de $\Lin{U, V}$ e as ($m\times{n}$)-matrizes escalares. Para todo $j = 1, \cdots, n$, temos
$Ab_j = \sum\limits_{i=1}^{m}a_{ij}c_{i}$ e $A'b_j = \sum\limits_{i=1}^{m}a'_{ij}c_{i}$, em que
$a_{ij}, a'_{ij}\in\mathbb{K}.$ Ent\~ao,
$$
	(A + A')b_j = Ab_j + A'b_j = \sum_{i=1}^{m}(a_{ij} + a'_{ij})c_{i}, \quad \text{\&} \quad
	(kA)b_j = kAb_j = \sum\limits_{i=1}^{m}(ka_{ij})c_{i}.
$$
Em outras palavras, $[A]^\beta_\gamma = [a_{ij}], [A']_\gamma^\beta, [A + A']^\beta_\gamma = [a_{ij} + a'_{ij}],
	\text{ e } [kA]_\gamma^\beta = k[A]_\gamma^\beta.$
Al\'em disso, escrevendo $[u]_\beta = \begin{bmatrix} k_{1} \\ k_{2} \\ \vdots \\ k_{n} \end{bmatrix} $,
ent\~ao
$$
	Au = \sum_{j=1}^{n}k_jAb_j = \sum_{j=1}^{n}k_j\biggl(\sum_{i=1}^{m}a_{ij}c_{i}\biggr)
	= \sum_{i=1}^{m}\biggl(\sum_{j=1}^{n}a_{ij}k_j\biggr)c_{i},
$$
de modo que, notando como $\sum\limits_{j=1}^{n}a_{ij}k_j$ \'e o i-\'esimo coeficiente da coluna da matriz
produto $[a_{ij}]\begin{bmatrix} k_{1} \\ k_{2} \\ \vdots \\ k_{n} \end{bmatrix}$, ent\~ao
$[Au]_\gamma = [A]_\gamma^\beta[u]_\beta$ para todos $u\in{U}, A\in{\Lin{U, V}}$. Traduzindo isso para
um portugu\^es leg\'ivel, acabamos de mostrar que a a\c c\~ao de uma transforma\c c\~ao linear num elemento
qualquer $u\in{U}$ \'e descrita pelo produto dos coeficientes dessa transforma\c c\~ao com a matriz dos coeficientes
de u!
Essa quest\~ao levanta um inqu\'erito importante - o que acontece com a composi\c c\~ao de duas transforma\c c\~oes?
Se considerarmos agora uma base $\delta: d_1, d_2, \cdots, d_l$ uma base linear de W, chamando a outra matriz de
$[B]_\delta^\gamma = [b_{si}]$, s\~ao dadas pelas igualdades $Ab_j = \sum\limits_{i=1}^{m}a_{ij}c_{i}$ para todo
$j = 1, \cdots, n$ e $Bc_i = \sum\limits_{s=1}^{l}b_{si}d_{s}$ para todo $i = 1, \cdots, m$, ent\~ao
$$
	(B\circ{A})b_j = B(Ab_j) = B(\sum_{i=1}^{m}a_{ij}c_{i}) = \sum_{i=1}^{m}a_{ij}Bc_i = \sum_{i=1}^{m}a_{ij}
	\biggl(\sum_{s=1}^{l}b_{si}d_s\biggr) = \sum_{s=1}^{l}\biggl(\sum_{i=1}^{m}b_{si}a_{ij}\biggr)d_s.
$$
Assim, o sj-\'esimo coeficiente da matriz $[B\circ{A}]_{\delta}^{\beta}$ \'e igual a $\sum\limits_{i=1}^{m}
	b_{si}a_{ij}$. Note que este coeficiente \'e o mesmo que o produto de duas matrizes, sendo elas
$[B\circ{A}]_{\delta}^{\beta} = [B]_{\delta}^{\beta}\cdot[A]_{\gamma}^{\beta}.$ Conclu\'imos disso que
a composta de aplica\c c\~oes lineares \'e o produto das matrizes de cada uma!

Com tudo isso, obtemos um exemplo particularmente importante de matriz, a chamada mudan\c ca de base:
\begin{def*}
	Seja V um $\mathbb{K}$-espa\c co vetorial e sejam $\beta, \gamma$ duas bases lineares de V. A matriz
	$M_\gamma^{\beta} := [1_V]_{\gamma}^{\beta}$ \'e chamada matriz de mudan\c ca de base $\beta$ para $\gamma$.
\end{def*}

\subsection{Funcionais Lineares e Espa\c co Dual}
Um tipo particular de aplica\c c\~oes lineares s\~ao os funcionais lineares, definidos a seguir
\begin{def*}
	Seja V um $\mathbb{K}$-espa\c co vetorial. Um funcional linear \'e um mapeamento $f: V\rightarrow{\mathbb{K}}$
	que satisfaz, para $u, v\in{V}, k\in\mathbb{K}$, as seguintes propriedades
	$$
		f[u + v] = f[u] + f[v]
		f[ku] = kf[u].
	$$
\end{def*}
Essencialmente, a diferen\c ca entre funcionais lineares e aplica\c c\~oes lineares \'e que, quando falando de
aplica\c c\~oes lineares, damos entrada de vetores para receber outros vetores no outro espa\c co, e no caso dos
funcionais, damos entrada de vetores para a fun\c c\~ao e obtemos escalares na imagem.
Em particular, o conjunto dos funcionais lineares de um espa\c co vetorial V forma um espa\c co sobre $\mathbb{K}$,
o chamado espa\c co dual $V^*$.
\begin{example}
	Considere V = $\mathbb{K}^n$. Podemos exibir um funcional linear
	$$
		f: \mathbb{K}^n\rightarrow{\mathbb{K}},
	$$
	$f(x_1, \cdots, x_n) = \sum\limits_{i=1}^{n}\alpha_ix_i, \alpha_1, \cdots, \alpha_n\in{\mathbb{K}}.$ H\'a um outro
	funcional $\pi: \mathbb{K}^{n}\rightarrow\mathbb{K}$ nesse espa\c co dado por $\pi(x_1, \cdots, x_n) = x_i$, a chamada
	i-\'esima proje\c c\~ao de $\mathbb{K}^{n}$ em $\mathbb{K}$.
	\qedsymbol
\end{example}
Um exemplo particularmente interessante j\'a \'e um velho amigo conhecido do c\'alculo diferencial e integral
\begin{example}
	Considere o espa\c co das fun\c c\~oes cont\'inuas de um intervalo $[a, b] \text{ em } \mathbb{C},
		\mathcal{C}([a, b], \mathbb{C}).$ Definimos um funcional linear $I:\mathcal{C}([a, b], \mathbb{C})\rightarrow
		\mathbb{C}$ dado por
	$$
		I(x) = \int_a^b x(t)dt.
	$$
	Em geral, note que integrais e derivadas podem ser vistas como aplica\c c\~oes lineares devido \`as suas propriedades
	de soma e multiplica\c c\~ao por escalar, enquanto a integral definida \'e um funcional, pois ela devolve um valor
	num\'erico como resultado.
\end{example}
Nosso pr\'oximo passo \'e construir uma base para o espa\c co dual tendo como refer\^encia a base do espa\c co
vetorial.
Seja $\beta: b_1, b_2, \cdots, b_n$ uma base de V. Para todo $j = 1, 2, \cdots, n$, podemos encontrar um
\'unico funcional linear $b_j^*: V\rightarrow\mathbb{K}$ dado por $b_j^*b_j = 1, b_j^*b_i = 0$ para $i\neq{j}$.
Com isso, $\beta^*: b_1^*, b_2^*, \cdots, b_n^*$ forma uma base para $V^*$, chamada base dual. A independ\^encia linear
n\~ao \'e complicada de ser mostrada, ent\~ao vamos mostrar que esse candidato a base gera o espa\c co. Com efeito,
seja $f\in{V^*}$. Temos
$$
	\biggl(\sum_{j=1}^{n}fb_jb_j^*\biggr) = \sum_{j=1}^{n}(fb_j)(b_j^*b_i) = fb_i (\text{ todas as outras parcelas s\~ao 0}).
$$
Logo, todo funcional pode ser escrito como uma combina\c c\~ao linear de elementos de $\beta^*$ de maneira \'unica,
tornando $\beta^*$ uma base.
Em suma, constru\'imos manualmente uma base para o espa\c co dual $V^*$ usando a base de V. Esta nomenclatura,
\textit{dual}, se origina do fato de que podemos escrever elementos de um espa\c co com base nos elementos do outro,
num processo formalizado no seguinte Teorema
\begin{theorem*}
	Seja V um $\mathbb{K}$-espa\c co de dimens\~ao finita com base $\beta: b_1, b_2, \cdots, b_n$. Ent\~ao, existe
	uma \'unica base $\beta^*: b_1^*, b_2^*, \cdots, b_n^*$ tal que $b_j^*b_i = 0, i\neq{j}, \& b_j^*b_j = 1$. Al\'em
	disto, para cada $v\in{V}$, temos
	$$
		v = \sum_{i=1}^{n}b_i^*(v)b_i
	$$
	e, para cada $f\in{V*}$,
	$$
		f = \sum_{i=1}^{n}f(b_i)b_i^*
	$$
\end{theorem*}
Emerge uma quest\~ao - \'e poss\'ivel fazer o caminho oposto? Dada uma base de $V^*$, conseguimos construir um
espa\c co dual a este, resultando em uma base $\beta$ de V? A resposta disso leva ao surgimento de um espa\c co
chamado bidual, definido a seguir
\begin{def*}
	Seja V um $\mathbb{K}$-espa\c co vetorial. Chamamos o espa\c co $(V^*)^*$ de espa\c co bidual a V e usamos
	a nota\c c\~ao $V^{**}$.
\end{def*}
Temos uma aplica\c c\~ao natural $I_V: V\rightarrow{V^{**}}$ agindo em $v\in{V}, f\in{V*}$ que \'e dada por
$(I_Vv)f:= f[v]\in{\mathbb{K}}$, ou seja, entendemos qualquer $v\in{V}$ como um funcional linear sobre $V^*$,
enviando $f\in{V^*}$ para $f[v]\in\mathbb{K}.$ Vamos conferir que o representante $I_Vv$ \'e linear:
$$
	(I_Vv)(f_1 + f_2) = (f_1 + f_2)[v] = f_1[v] + f_2[v] = (I_Vv)(f_1) + (I_Vv)(f_2), \quad
	(I_Vv)(kf_1) = (kf_1)[v] = k(I_Vv)(f_1)
$$
para $f_1, f_2\in{V*}, k\in\mathbb{K}, v\in{V}$. Precisamos agora mostrar que a aplica\c c\~ao $I_V$ \'e linear.
Para isso, tome $v_1, v_2\in{V}, f\in{V^*}, k\in\mathbb{K}$. Assim,
$$
	(I_V(v_1 + v_2))f = f[v_1 + v_2] = f[v_1] + f[v_2] = (I_Vv_1)f + (I_Vv_2)f, \quad
	(I_V(kv_1))f = f[kv_1] = kf[v_1] = k(I_Vv_1)f.
$$
Portanto, esta aplica\c c\~ao \'e linear entre os espa\c cos. No entanto, qual \'e o sentido de fazermos essa
constru\c c\~ao? A resposta \'e que ela prov\'em um isomorfismo entre $V^{**}$ e V, ou seja, o espa\c co bidual
\'e igual ao espa\c co original!! (N\~ao ser\'a provado, acredito que saia do escopo das notas, essa \'ultima
constru\c c\~ao \'e mais a t\'itulo de curiosidade.)

\section{Polin\^omios Caracter\'isticos}

\end{document}

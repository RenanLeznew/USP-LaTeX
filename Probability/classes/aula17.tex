\documentclass[probability_notes.tex]{subfiles}
\begin{document}

\section{Aula 17 - 07/12/2023}
\subsection{Motivações}
\begin{itemize}
	\item Função Massa de Probabilidade. Marginal;
	\item Função Massa de Probabilidade Condicional;
\end{itemize}
\subsection{Função Massa de Probabilidade Marginal}
\begin{example}
	Continuando o exemplo, podemos encontrar a função massa de probabilidade marginal na variável x fazendo
	\[
		f(x) = \int_{0}^{1}2(x+y - 2xy)dy = [2xy + y^{2} - xy^{2}]\biggl|_{0}^{1}\biggr. = 2x + 1 - 2x = 1.
	\]
	Analogamente, a de y será
	\[
		f(y) = \int_{0}^{1}2(x+y-2xy)dx = \biggl[x^{2} + 2xy - 2x^{2}y\biggr]\biggl|_{0}^{1}\biggr. = 1 + 2y - 2y = 1.
	\]
	Concluimos, assim, que x e y são distribuídas uniformemente em \([0, 1].\)
\end{example}
\subsection{Condicionalidade e Independência das Funções Massas de Probabilidade}
\begin{def*}
	Seja o vetor aleatório bidimensional (X, Y). Então,
	\[
		p(x|y) = \mathbb{P}(X=x|Y = y) = \frac{p(x, y)}{q(y)},\quad \text{se }q(y) > 0,
	\]
	e
	\[
		q(y| x) = \mathbb{P}(Y=y| X=x) = \frac{p(x, y)}{p(x)},\quad \text{se }p(x) > 0.\quad \square
	\]
\end{def*}
\begin{example}
	Considere o seguinte modelo:
	\begin{align*}
		Y| & x\sim \mathrm{Poisson}(x)                                      \\
		   & x\sim \mathrm{Gamma}(\alpha , \beta ),\quad \alpha , \beta >0.
	\end{align*}
	Nesse caso, a probabilidade conjunta de \((Y, X)\) é descrita por
	\begin{align*}
		f(y, x) & = \mathbb{P}(Y=y| X=x)f(x)                                                                 \\
		        & = \frac{e^{-x}x^{y}}{y!}\frac{\beta ^{\alpha }}{\Gamma (\alpha )}x^{\alpha -1}e^{-\beta x} \\
		        & = \frac{\beta ^{\alpha }}{y!\Gamma (\alpha )}x^{(\alpha +y)-1}e^{-(\beta +1)x}.
	\end{align*}
	Pela definição da função condicional, segue que
	\begin{align*}
		\mathbb{P}(Y=y) & = \int_{0}^{\infty}f(x, y)dx                                                                                                                   \\
		                & = \frac{\beta ^{\alpha }}{y!\Gamma (\alpha )}\int_{0}^{\infty}e^{x}x^{y}x^{\alpha -1}e^{-\beta x}dx                                            \\
		                & = \frac{\beta ^{\alpha }}{y!\Gamma (\alpha )}\frac{\Gamma (\alpha +y)}{(\beta +1)^{\alpha +y}}                                                 \\
		                & = \biggl(\frac{\beta }{\beta +1}\biggr)^{\alpha }\biggl(\frac{1}{\beta +1}\biggr)^{y} \frac{\Gamma (\alpha +y)}{\Gamma (\alpha )}\frac{1}{y!}.
	\end{align*}
\end{example}
\begin{example}
	Vejamos outro caso similar. Considere que
	\begin{align*}
		Y| & x\sim \mathrm{Binomial}(n;x)          \\
		   & x\sim \mathrm{Beta}(\alpha , \beta ).
	\end{align*}
	Neste caso,
	\begin{align*}
		f(x, y) & = \mathbb{P}(Y=y|x)f(x)                                                                                                      \\
		        & = \binom{n}{y}x^{y}(1-x)^{wn-y}\frac{\Gamma (\alpha +\beta) }{\Gamma (\alpha )\Gamma (\beta )}x^{\alpha -1}(1-x)^{\beta -1}.
	\end{align*}
	Consequentemente, tiramos disso que
	\begin{align*}
		\mathbb{P}(Y=y) & = \int_{0}^{1}f(y, x)dx                                                                                                                                                 \\
		                & = \binom{n}{y}\frac{\Gamma (\alpha +\beta )}{\Gamma (\alpha )\Gamma (\beta )}\int_{0}^{1}x^{(y+\alpha )-1}(1-x)^{(n+\beta -1)-1}dx                                      \\
		                & = \binom{n}{y}\frac{\Gamma (\alpha +\beta )}{\Gamma (\alpha )\Gamma (\beta )}\underbrace{\frac{\Gamma (y+\alpha )\Gamma (n-y+\beta )}{\Gamma (n+\beta +\alpha )}}_{(*)} \\
		                & = \frac{\Gamma (n+1)}{\Gamma (y+1)\Gamma (n-y+1)}\cdot *
	\end{align*}
\end{example}
\begin{def*}
	Seja (X, Y) um vetor aleatório discreto e bidimensional. Dizemos que X e Y são independentes se, e somente se,
	\[
		p(x, y) = p(x)p(y)
	\]
	para quaisquer x e y. Em outras palavras, \(\mathbb{P}(X=x, Y=y) = \mathbb{P}(X=x)\mathbb{P}(Y=y)\) para todo x, y.
\end{def*}
\begin{example}
	Uma máquina é utilizada para uma determinada tarefa pela manhã e outra pela tarde, sendo X e Y os números
	de vezes que a máquina é interrompida por algum problema pela manhã e pela tarde, respectivamente.

	\begin{center}
		\begin{table}[h!]
			\centering
			\begin{tabular}{| c | c  c  c | c |}
				\hline
				\(y\setminus x\) & 0    & 1    & 2    & q(y) \\
				\hline
				0                & 0,10 & 0,20 & 0,20 & 0,50 \\
				1                & 0,04 & 0,08 & 0,08 & 0,20 \\
				2                & 0,06 & 0,12 & 0,12 & 0,30 \\
				\hline
				p(x)             & 0,20 & 0,40 & 0,40 & 1,00 \\
				\hline
			\end{tabular}
		\end{table}
		Então, temos
		\begin{align*}
			 & 0,10 = 0,50\times 0,20 = p(0)p(0) = p(0, 0) \\
			 & 0,20 = 0,50\times 0,40 = p(1)p(0) = p(1, 0) \\
			 & 0,20 = 0,50\times 0,40 = p(2)p(0) = p(2, 0) \\
			 & \vdots                                      \\
		\end{align*}
		do que segue que X e Y são independentes.
	\end{center}
\end{example}
\begin{example}
	Sejam X e Y a duração da vida de dois dispositivos eletrônicos e suponha que sua função densidade de probabilidade conjunta seja dada por
	\[
		f(x, y) = e^{-(x+y)},\quad x\geq 0,y\geq 0.
	\]
	A independência segue pois podemos fatorar \(e^{-(x+y)} = e^{-x}e^{-y}\).
\end{example}
\subsection{Funções de Variáveis Aleatórias}
Considere a variável aleatória Z definida como uma função de um vetor aleatório bidimensional (X, Y), ou seja, \(Z = H_{1}(X, Y).\)
O problema que se coloca é
\begin{quote}
	``Dada a função que descreve a probabilidade conjunta de (X, Y), qual é a função que descreve a probabilidade de \(Z = H_{1}(X, Y)\)?''
\end{quote}

\subsubsection{!!!!IMPORTANTE!!!!}
\textbf{A atividade consiste em escolher um desses dois modelos (Binomial-Beta ou Gama-Poisson), fazer um gráfico, descrevê-lo, encontrar a FGM, para que ele serve, etc.}
\end{document}

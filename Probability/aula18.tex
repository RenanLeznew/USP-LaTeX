\documentclass[main.tex]{subfiles}
\begin{document}
\section{Aula 18 - 12/12/2023}
\subsection{Motivações}
\begin{itemize}
	\item Funções de Variáveis Aleatórias;
	\item Teorema Central do Limite.
\end{itemize}
\subsection{Funções de Variáveis Aleatórias}
Para propor uma função densidade de probabilidade de \(Z = H_{1}(X, Y),\) é mais
simples introduzir uma segunda variável aleatória, por exemplo \(H_{2}(X, Y) = W\) e obter,
primeiro, a função densidade de probabilidade conjunta do par \((Z, W),\) digamos que ela seja \(k(z, w).\)

Com o conhecimento de \(k(z, w),\) poderemos obter, então, a função densidade de probabilidade de Z, i.e.,
g(z), fazendo a integral
\[
	g(z) = \int_{-\infty}^{\infty}k(z, w)dw.
\]

Suponhamos que (X, Y) seja uma variável aleatória contínua bidimensional com função densidade conjunta f. Sejam
\[
	Z = H_{1}(X, Y)\quad\&\quad W = H_{2}(X, Y).
\]
Admitimos que as funções \(H_{1, 2}\) satisfaçam a existência das derivadas parciais e a continuidade das mesmas e que

Nessas condições,
\[
	k(z, w) = f[G_1(z, w), G_{2}(z, w)]|J(z, w)|
\]
em que
\[
	J(z, w) = \begin{bmatrix}
		\frac{\partial^{}x}{\partial z^{}} & \frac{\partial^{}x}{\partial w^{}} \\
		\frac{\partial^{}y}{\partial z^{}} & \frac{\partial^{}y}{\partial w^{}}
	\end{bmatrix}
\]
é a matriz Jacobiana.
\begin{example}
	Sejam \(X_{1}, X_{2}\) variáveis aleatórias exponenciais e independentes com média \(\frac{1}{\lambda } > 0.\) Encontre a função de densidade de
	\[
		U_{1}=\frac{X_{1}}{X_{1}+X_{2}}\quad\&\quad U_{2} = X_{1} + X_2.
	\]

	Para começar, observemos que
	\[
		X_{1}\sim \mathrm{Exp}(\lambda )\quad\&\quad X_{2}\sim \mathrm{Exp}(\lambda).
	\]
	Consequentemente,
	\[
		f_{X_{1}}(x_{1}) = \frac{1}{\lambda }e^{-\lambda x_{1}}\quad\&\quad f_{X_{2}}(x_{2}) = \lambda e^{-\lambda x_{2}}
	\]
	A densidade conjunta, então, é o produto delas, visto que são independentes, ou seja,
	\[
		f_{X}(x_{1},x_{2}) = \lambda^{2}e^{-\lambda (x_{1}+x_{2})}.
	\]
	A soma de distribuições exponenciais, como vimos, resulta numa distribuição Gamma. No entanto, nunca nos deparamos
	com um quociente como o de \(U_{1}.\) Para encontrar a distribuição resultante, encontramos uma forma de escreve \(X_{1}\) e \(X_{2}\)
	em termos de \(U_{1}\) e \(U_{2}\). Segue que
	\[
		X_{1} = U_{1}U_{2}\quad\&\quad X_{2} = U_{2} (1-U_{1}).
	\]
	Calculando a Jacobiana da inversa, segue que
	\[
		J(X_{1}, X_{2}) = \begin{bmatrix}
			\frac{\partial^{}X_{1}}{\partial U_{1}^{}} & \frac{\partial^{}X_{1}}{\partial U_{2}^{}} \\
			\frac{\partial^{}X_{2}}{\partial U_{1}^{}} & \frac{\partial^{}X_{2}}{\partial U_{2}^{}}
		\end{bmatrix}
		= \begin{bmatrix}
			U_{2}  & U_{1}      \\
			-U_{2} & 1 - U_{1}.
		\end{bmatrix}
	\]
	Com isso, o módulo da Jacobiana será
	\[
		|J| = U_2(1-U_{1}) + U_{2}U_{1} = U_{2}.
	\]
	Agora, podemos encontrar a função densidade de probabilidade conjunta:
	\[
		f_{U}(U_{1}, U_{2}) = \lambda ^{2}U_{2}e^{-\lambda U_{2}}.
	\]
	Finalmente, encontramos as funções densidade de probabilidade indivíduais integrando cada uma com respeito à outra, o que equivale a
	\begin{align*}
		f_{U_{1}}(U_{1}) & = \int_{0}^{\infty}f_{U}(U_{1}, U_{2}) dU_{2}                                                                                                                  \\
		                 & = \int_{0}^{\infty}\lambda ^{2}U_{2}e^{-\lambda U_{2}}dU_{2}                                                                                                   \\
		                 & = \lambda ^{2}\biggl[-\frac{U_{2}e^{-\lambda U_{2}}}{\lambda }\biggl|_{0}^{\infty}\biggr. - \frac{1}{\lambda }\int_{0}^{\infty}e^{-\lambda U_{2}}dU_{2}\biggr] \\
		                 & = \lambda ^{2}\biggl[ 0 + \frac{1}{\lambda^{2}} \biggr]                                                                                                        \\
		                 & = 1,\quad 0 < U_{1} < 1.
	\end{align*}
	Logo, \(U_{1}\) segue uma distribuição uniforme! Quanto à \(U_{2}\), sabemos que ela será distribuição Gamma. No entanto,
	não sabemos quais são os parâmetros. Para encontrá-los, note que
	\begin{align*}
		f_{U_{2}}(U_{2}) & = \int_{0}^{1}\lambda ^{2} U_{2}e^{-\lambda U_{2}}dU_{1}                                      \\
		                 & = \lambda ^{2} U_{2}e^{-\lambda U_{2}}U_{1}\biggl|_{1}^{0}\biggr.                             \\
		                 & = \underbrace{\lambda ^{2}U_{2}e^{-\lambda U_{2}}}_{\text{Núcleo Gamma (2, }\lambda \text{)}}
	\end{align*}
\end{example}
\subsection{O Teorema Central do Limite}
\begin{theorem*}
	Seja \(X_{1}, X_2, \dotsc \) uma sequência de variáveis aleatórias independentes e identicamente distribuídas com média \(\mu \) e variância
	\(\sigma ^{2}\). Seja \(S_{n} = X_{1} + X_{2} + \dotsc +X_{n}\). Então, \(Z_{n} = \frac{S_{n} -n\mu }{\sigma \sqrt[]{n}}\) converge em distribuição para
	\(Z\sim \mathrm{Normal}(0, 1)\)
\end{theorem*}
Considere um conjunto de ariáveis aleatórias independentes e identicamente distribuídas, \(X_{1}, X_{2}, \dotsc , X_{n},\) em que \(\mathbb{E}(X_{i}) = \mu\) e
\(\mathrm{Var}(X_{i}) = \sigma^{2} < \infty,\) para \(i = 1, \dotsc , n.\)

Defina a variáveis
\[
	U_{n} = \frac{\overline{X} - \mu}{\frac{\sigma }{\sqrt[]{n}}}\quad\&\quad \overline{X} = \frac{1}{n}\sum\limits_{i=1}^{n}X_{i}.
\]
Então, quando \(n\to\infty\), a função de distribuição \(U_{n}\) converge para a função de distribuição de uma variável aleatória
normal padrão. Em outras palavras,
\[
	\lim_{n\to \infty}\mathbb{P}(U_{n}\leq u) = \int_{-\infty}^{u}\frac{1}{\sqrt[]{2\pi }}e^{-\frac{t^{2}}{2}}dt,
\]
para qualquer u real. Em particular, note que isso é uma distribuição composta do tipo
\[
	U_{n}\sim \mathrm{Normal}(0, 1)\quad\&\quad \overline{X}\sim \mathrm{Normal}\biggl(\mu, \frac{\sigma ^{2}}{n}\biggr)
\]
A justificativa para isso é como segue: Seja
\[
	U_{n} = \frac{\overline{X} - \mu }{\frac{\sigma }{\sqrt[]{n}}} = \frac{1}{\sqrt[]{n}}\biggl(\frac{\sum\limits_{i=1}^{n}X_{i} - n\mu }{\sigma }\biggr) = \frac{1}{\sqrt[]{n}}\sum\limits_{i=1}^{n}Z_{i},
\]
em que \(Z_{i} = \frac{X_{i}-\mu}{\sigma }\).

Como as variáveis são i.i.d's, então as \(Z_{i}\)'s também serão, com \(i=1, 2, \dotsc , n\) e \(\mathbb{E}(Z_{i}) = 0\) e \(\mathrm{Var}(Z_{i}) = 1.\)
Pelo Teorema de Taylor, com resto
\[
	M_{Z_{1}}(t) = M_{Z_{1}}(0) + M_{Z_1}'(0)t + \frac{M_{Z_{1}}''(\xi)}{2}t^{2} = 1 + \frac{1}{2}M_{Z_{1}''(\xi)}t^{2},\quad 0 < \xi < t
\]
Note que a FGM de \(U_{n}\) será dada por
\[
	M_{U_{n}}(t) = \biggl(1 + \frac{M_{Z_{1}}''(\xi_{n})t^{2}}{2n}\biggr)^{n}.
\]

Agora, notem que \(\xi_{n}\to 0 \) quando \(n\to \infty\) e, portanto,
\[
	M_{Z_{1}''}(\xi_{n})\frac{t^{2}}{2}\to M_{Z_{1}}''(0)\frac{t^{2}}{2} = \mathbb{E}(Z_{1}^{2})\frac{t^{2}}{2} = \frac{t^{2}}{2}.
\]
Assim,
\[
	\lim_{n\to \infty}M_{U_{n}}(t) = \lim_{n\to \infty}\biggl(1 + \frac{M_{Z_{1}}''(\xi_{n})t^{2}}{2n}\biggr)^{n} = e^{\frac{t^{2}}{2}},
\]
que é a função geradora de momentos de uma variável aleatória normal padrão.
\begin{example}
	As pontuações de determinado teste de todos os alunos do último ano do ensino médio em um estado têm médioa 60 e variância 64. Uma amostra aleatória de n=100 alunos de uma grande
	escola teve pontuação média igual a 58. Há evidências sugerindo que esta escola seja inferior? Calcule a probabilidade de que a média da amostra seja no máximo 58 quando \(n = 100\).

	Para isto, temos \(\mu = 60, \sigma ^{2} = 64\) e \(\overline{X} = 58.\) Buscamos ver quão provável é obter 58 numa amostra de 100 alunos. Se a probabilidade for baixa,
	a escola será inferior e, caso contrário, não será. Neste prisma, a conta que faremos é
	\[
		\frac{\overline{X} - \mu }{\sigma/\sqrt[]{n}} = \frac{58 - 60}{8/10} = -2,5.
	\]
	Mas, por ter esta distribuição, segue que \(U_{n}\sim \mathrm{Normal}(0, 1)\). Assim, buscamos descobrir o quão provável é encontrar um valor
	\(-2,5\) na normal padrão. Observar este valor nela é altamente improvável, ou seja, a escola é péssima com relação à média populacional, pois
	\[
		\mathbb{P}(\overline{X}\leq 58) = \mathbb{P}\biggl(\frac{\overline{X}-60}{8/10} \leq \frac{58 - 60}{0,8}\biggr)\approx 0,0062.
	\]
\end{example}
\begin{example}
	Os tempos de serviço para clientes que passam por um caixa em uma loja de varejo são variáveis aleatórias independentes com média de 1,5 minutos e variância de 1,0.
	Aproxime a probabilidade de que 100 clientes possam ser atendidos em menos de 2 horas de tempo total de serviço.

	Neste caso,
	\[
		\frac{1,2 - 1,5}{1/10} = -3.
	\]
	Novamente, concluimos que é MUITO improvável que isso aconteça. Isto decorre do fato da variância ser alta, o que dificulta o antedimento. De fato,
	\[
		\mathbb{P}(\overline{X} \leq 120) = \mathbb{P}(Z\leq -3)\approx 0,0013.
	\]
\end{example}
Outra utilidade do Teorema Central do Limite é que permite aproximar outras distribuições por meio da normal.
Suponha que \(Y\sim \mathrm{Bin}(25; 0,4)\). Encontre as probabilidade de que \(Y\leq 8\) e \(Y = 8\).
\end{document}

\documentclass[measure_theory.tex]{subfiles}
\begin{document}
\section{Aula 14 - 05/02/2024}
\subsection{Motivações}
\begin{itemize}
	\item Espaços de Sobolev;
	\item Distribuições;
	\item Teoria Espectral.
\end{itemize}
\subsection{Hugo O. B. - Espaços de Sobolev}
\begin{def*}
	Seja \(C_{K}^{\infty}\) o conjunto de todas as funções em \(\mathbb{R}^{n}\) que possuem suporte compactos e que são infinitamente diferenciáveis. Para \(j=(j_{1},\dotsc ,j_{n})\), escreva
	\[
		D^{j}f=\frac{\partial^{j_{1}+\dotsc +j_{n}}f}{\partial ^{j_{1}}_{x_{1}}\dotsc \partial ^{j_{n}}_{x_{n}}}
	\]
	e defina \(|j|=j_{1}+\dotsc +j_{n}.\) Assuma que f, g são localmente integráveis. Então, diremos que \(D^{j}f=g\) \textbf{fracamente}, ou que \textbf{g é a j-ésima derivada parcial fraca de f}, se
	\[
		\int_{}^{}f(x)D^{j}\varphi (x)dx=(-1)^{|j|}\int_{}^{}g(x)\varphi (x)dx,\quad \forall \varphi\in C_{K}^{\infty}.\quad \square
	\]
\end{def*}

Note que, caso \(g=D^{j}f\) no sentido usual, então a integração por partes mostra que g também é a derivada fraca de f.

Iremos construir um espaço normado de funções a valores em \(\mathbb{R}^{n}\) que terão todas as derivadas até certa ordem pertencentes em \(L^{p}\). Dentro desse espaço, seremos capazes de estimar a norma-q das funções por meio do seu gradiente, além de explorar outras propriedades. Deste princípio, coloque
\[
	W^{k, p}(\mathbb{R}^{n})=\{f: f\in L^{p}, D^{j}f\in L^{p} \text{ para todo }j\text{ tal que }|j|\leq k\},
\]
com a norma
\[
	\Vert f \Vert_{W^{k, p}}=\sum\limits_{\{j:0\leq |j|\leq k\}}^{}\Vert D^{j}f \Vert_{p},
\]
em que colocamos \(D^{0}f=f.\)
\begin{theorem*}
	O espaço \(W^{k, p}\) é completo.
\end{theorem*}
\begin{proof*}
	Seja \(f_{m}\) uma sequência de Cauchy em \(W^{k, p}\). Para cada j com \(|j|\leq k\), vemos que \(D^{j}f_{m}\) é uma sequência de Cauchy em \(L^{p}\). Sendo \(L^{p}\) completo, existe \(g_{j}=\lim_{m\to \infty}D^{j}f_{m}\) em \(L^{p}\) e \(f=\lim_{m\to \infty}f_{m}\), também em \(L^{p}\). Então,
	\[
		\int_{}^{}f_{m}D^{j}\varphi =(-1)^{|j|}\int_{}^{}(D^{j}f_{m})\varphi \longrightarrow (-1)^{|j|}\int_{}^{}g_{j}\varphi.
	\]
	para todo \(\varphi \) em \(C_{K}^{\infty}\). Por outro lado,
	\[
		\int_{}^{}f_{m}D^{j}\varphi \overbracket[0pt]{\longrightarrow}^{m\to \infty}\int_{}^{}f D^{j}\varphi,
	\]
	tal que, comparando as duas igualdades,
	\[
		(-1)^{|j|}\int_{}^{}g_{j}\varphi = \int_{}^{}fD^{j}\varphi
	\]
	para todo \(\varphi \in C_{K}^{\infty}.\) Logo, \(g_{j}=D^{j}f\) quase sempre para todo j que satisfaça \(|j|\leq k.\) Portanto, \(D^{j}f_{m}\) converge para \(D^{j}f\) em \(L^{p}\) para todo j tal que \(|j|\leq k\), concluindo a demonstração. \qedsymbol
\end{proof*}
Dado o espaço que iremos trabalhar, construiremos as etapas para obter e provar a \textbf{Desigualdade de Sobolev}, mencionada anteriormente.
\begin{lemma*}
	Se \(k\geq 1\) e \(f_{1},\dotsc , f_{k}\geq 0\), então
	\[
		\int_{}^{}f_{1}^{\frac{1}{k}}\cdots f_{k}^{\frac{1}{k}}\leq \biggl(\int_{}^{}f_{1}\biggr)^{\frac{1}{k}}\cdots \biggl(\int_{}^{}f_{k}\biggr)^{\frac{1}{k}}.
	\]
\end{lemma*}
\begin{proof*}
	Vamos provar o análogo à desigualdade, mas elevando tudo a k. A prova será por indução, com o caso \(k=1\) sendo
	\[
		\int_{}^{}f_{1}=\int_{}^{}f_{1}.
	\]
	Suponha, então, que a desigualdade é válida para k-1, tal que é válida a desigualdade
	\[
		\biggl(\int_{}^{}f_{1}^{\frac{1}{(k-1)}}\cdots f_{k-1}^{\frac{1}{k-1}}\biggr) \leq \biggl(\int_{}^{}f_{1}\biggr)\cdots \biggl(\int_{}^{}f_{k-1}\biggr).
	\]
	Seja \(p=\frac{k}{(k-1)}\) e \(q=k\), tal que
	\[
		\frac{1}{p}+\frac{1}{q}=1.
	\]
	Pela \hyperlink{holder}{\textit{Desigualdade de Hölder}},
	\[
		\int_{}^{}(f_{1}^{\frac{1}{k}}\cdots f_{k-1}^{\frac{1}{k}})f_{k}^{\frac{1}{k}}\leq \biggl(\int_{}^{}f_{1}^{\frac{1}{k-1}}\cdots f_{k-1}^{\frac{1}{k-1}}\biggr)^{\frac{(k-1)}{k}}\biggl(\int_{}^{}f_{k}\biggr)^{\frac{1}{k}}.
	\]
	Elevando ambos os lados à k-ésima potência resultará em
	\[
		\biggl(\int_{}^{}(f_{1}^{\frac{1}{k}}\cdots f_{k-1}^{\frac{1}{k}})f_{k}^{\frac{1}{k}}\biggr)\leq \biggl(\int_{}^{}f_{1}^{\frac{1}{k-1}}\cdots f_{k-1}^{\frac{1}{k-1}}\biggr)^{(k-1)}\biggl(\int_{}^{}f_{k}\biggr).
	\]
	Pela hipótese indutiva, substituindo a desigualdade válida fornece o resultado que buscamos. \qedsymbol
\end{proof*}
Começaremos provando a \textbf{Desigualdade de Gagliardo-Nirenberg}, que é uma versão do que queremos, mas para a norma de \(L^{1}\):
\hypertarget{gagliardo-nirenberg-inequality}{
	\begin{theorem*}[Desigualdade de Gagliardo-Nirenberg]
		Existe uma constante \(c_{1}\) dependendo apenas de n tal que, se \(u\in C_{K}^{1}\), então
		\[
			\Vert u \Vert_{\frac{n}{n-1}}\leq c_{1}\Vert |\nabla u| \Vert_{1}
		\]
	\end{theorem*}
}
É importante u ter suporte compacto, pois, caso contrário, seria possível escolher \(u\equiv 1\) e chegar a uma contradição.
\begin{proof*}
	Coloque \(s=\frac{1}{(n-1)}\). Seja \(K_{j_{1}\dotsc j_{m}}\) a integral de \(|\nabla u(x_{1}, \dotsc , x_{n})|\) com respeito às variáveis \(x_{j_{1}}, \dotsc , x_{j_m}\). Desta forma,
	\[
		K_{1}=\int_{}^{}|\nabla u(x_{1}, \dotsc , x_{n})|dx_{1},\quad K_{23}=\iint_{}|\nabla u(x_{1}, \dotsc , x_{n})| dx_{2}dx_{3}, \dotsc
	\]
	Note que \(K_{1}\) é uma função de \((x_{2}, \dotsc , x_{n})\) e \(K_{23}\) é uma função de \((x_{1}, \dotsc , x_{4})\) - em outras palavras, para \(K_{j_{1}\dotsc j_{n}}\), como será uma integral com respeito a \((x_{j_1},\dotsc , x_{j_{n}})\), a função perde a dependência nessas variáveis, mas continua de todas as outras.

	Se \(x=(x_{1},\dotsc ,x_{n})\in \mathbb{R}^{n}\), então, como u tem suporte compacto,
	\begin{align*}
		|u(x)| & =\biggl\vert \int_{-\infty}^{x_{1}}\frac{\partial^{}u}{\partial x_{1}^{}}(y_{1), x_{2}, \dotsc , x_{n}})dy_{1} \biggr\vert \\
		       & \leq \int_{\mathbb{R}}^{}|\nabla u(y_{1}, x_{2}, \dotsc , x_{n})|dy_{1}                                                    \\
		       & =K_{1}.
	\end{align*}
	Pelo mesmo argumento, podemos substituir \(x_{1}\) e \(y_{1}\) por \(x_{i}\) e \(y_{i}\) para cada i natural, de maneira que \(|u(x)|\leq K_{i}\) e
	\[
		|u(x)|^{\frac{n}{(n-1)}}=|u(x)|^{ns}\leq K_{1}^{s}K_{2}^{s}\dotsc K_{n}^{s}.
	\]
	Como o \(K_{1}\) não depende de \(x_{1}\), segue que
	\begin{align*}
		\int_{}^{}|u(x)|^{ns}dx_{1} & \leq K_{1}^{s}\cdots K_{n}^{s}dx_{1}                                                                 \\
		                            & \leq K_{1}^{s}\biggl(\int_{}^{}K_{2}dx_{1}\biggr)^{s}\cdots \biggl(\int_{}^{}K_{n}dx_{1}\biggr)^{s}.
	\end{align*}
	Observe como
	\[
		\int_{}^{}K_{2}dx_{1}=\int_{}^{}\biggl(\int_{}^{}|\nabla u(x_{1}, \dotsc , x_{n})|dx_{2}\biggr) = K_{12},
	\]
	tal que, repetindo isso para outros valores além de 2, obtemos
	\[
		\int_{}^{}|u|^{ns}dx_{1}\leq K_{1}^{s}K_{12}^{s}\cdots K_{1n}^{s}.
	\]
	Assim, como \(K_{12}\) não depende de \(x_{2}\),
	\begin{align*}
		\int_{}^{}|u(x)|^{ns}dx_{1}dx_{2} & \leq K_{12}^{s}\int_{}^{}K_{1}^{s}K_{13}^{s}\cdots K_{1n}^{s}dx_{2}                                                                                                                           \\
		                                  & K_{12}^{s}\biggl(\int_{}^{}K_{1}dx_{2}\biggr)^{s}\biggl(\int_{}^{}K_{13}dx_{2}\biggr)^{s}\cdots \biggl(\int_{}^{}K_{1n}dx_{2}\biggr)^{s} = K_{12}^{s}K_{12}^{s}K_{123}^{s}\cdots K_{12n}^{s}.
	\end{align*}
	Repetindo este processo fornece
	\[
		\int_{}^{}|u(x)|^{ns}dx_{1}dx_{2}dx_{3}\leq K_{123}^{s}K_{123}^{s}K_{123}^{s}K_{1234}^{s}\cdots K_{123n}^{s}.
	\]
	Continuando ainda mais, chegamos em
	\[
		\int_{}^{}|u(x)|^{ns}dx_{1}\cdots dx_{n}\leq (K_{12\cdots n}^{s})^{n}=K_{12\cdots n}^{ns}.
	\]
	Portanto, lembrando que \(s=\frac{1}{n-1}\) e tomando a \(ns\)-ésima raíz dos dois lados, obtemos o que queríamos. \qedsymbol
\end{proof*}

Finalmente, podemos obter as \textbf{Desigualdades de Sobolev}
\hypertarget{sobolev_inequality}{
	\begin{theorem*}[Desigualdades de Sobolev]
		Suponha que \(1\leq p < n \) e \(u\in C_{K}^{1}.\) Então, existe uma constante \(c_{1}\) dependendo apenas de n e tal que
		\[
			\Vert u \Vert_{\frac{np}{(n-p)}}\leq c_{1}\Vert |\nabla u| \Vert_{p}.
		\]
	\end{theorem*}
}
\begin{proof*}
	O caso em que \(p=1\) é o caso anterior, então podemos supor \(1<p<n.\) Se \(u\equiv 0\), não há nada a ser feito. Tirando estes dois, seja
	\[
		r=\frac{p(n-1)}{n-p}
	\]
	e observe que \(k>1\),
	\[
		r-1=\frac{np-n}{n-p}.
	\]
	Coloque \(w=|u|^{r}\); como \(r>1\), então a função caracterizada por \(x\rightarrow |x|^{r}\) é continuamente diferenciável, tal que \(w\in C _{K}^{1}\). Observe que
	\[
		|\nabla w|\leq c_{2}|u|^{r-1}|\nabla u|.
	\]
	uma aplicação conjunta da \hyperlink{holder}{\textit{Desigualdade de Hölder}} com o teorema anterior para \(q=\frac{p}{p-1}\), obtemos
	\begin{align*}
		\biggl(\int_{}^{}|w|^{\frac{n}{n-1}}\biggr)^{\frac{n-1}{n}} & \leq c_{3}\int_{}^{}|\nabla w|                                                                                                         \\
		                                                            & \leq c_{4}\int_{}^{}|u|^{\frac{(np-n)}{(n-p)}}|\nabla u|                                                                               \\
		                                                            & \leq  c_{5}\biggl(\int_{}^{}|u|^{\frac{np}{(n-p)}}\biggr)^{\frac{p-1}{p}}\biggl(\int_{}^{}|u|^{\frac{np}{(n-p)}}\biggr)^{\frac{1}{p}}.
	\end{align*}
	Note que o lado esquerdo é, revertendo a substituição de w,
	\[
		\biggl(\int_{}^{}|u|^{\frac{np}{(n-p)}}\biggr)^{\frac{n-1}{n}}.
	\]
	Agora, dividindo ambos os lados por
	\[
		\biggl(\int_{}^{}|u|^{\frac{np}{(n-p)}}\biggr)^{\frac{p-1}{p}}
	\]
	e, como
	\[
		\frac{n-1}{n}-\frac{p-1}{p}=\frac{1}{p}-\frac{1}{n}=\frac{n-p}{pn},
	\]
	obtemos, portanto, o resultado desejado. \qedsymbol
\end{proof*}

É possível fazer a prova generalizando da função continuamente diferenciável com suporte compacto u para uma função f no espaço de Sobolev \(W^{k, p}\) por meio de iterações, de forma que o teorema a seguir é verídico, relacionando a q-norma de \(D^{k}f\) com a p norma de f:
\begin{theorem*}
	Suponha que \(k\geq 1\), \(p<\frac{n}{k}\) e que definimos q por \(\frac{1}{q}=\frac{1}{p}-\frac{k}{n}.\) Então, existe \(c_{1}\) tal que
	\[
		\Vert f \Vert_{q}\leq c_{1} \biggl\Vert \sum\limits_{\{j: |j|=k\}}^{}|D^{j}f| \biggr\Vert .
	\]
\end{theorem*}
Uma última observação é que é possível provar que, se \(p>\frac{n}{k}\) e \(f\in W^{k, p}\), então f é Hölder contínua.

\subsection{Renan W. - Distribuições}
As distribuições são ferramentas que aparecem em cada vez mais áreas. Dentre elas, cargas pontuais no eletromagnetismo são descritas por meio das distribuições,
formulações de teoria quântica dos campos e processamento de sinais valem a pena serem mencionadas. Numa perspectiva mais voltada à matemática, elas são utilizadas especialmente
para a solução de equações diferenciais, principalmente porque uma das suas utilidades é a definição e manipulação de derivadas de funções puramente contínuas, que não eram deriváveis antes.
Entender estes objetos é o foco desta apresentação, assim como a descrição de alguns resultados fundamentais.


Vamos nos restringir ao caso real, mas o conteúdo que será visto pode ser naturalmente estendido a \(\mathbb{R}^{n}.\) O primeiro passo é descrever um espaço legal de trabalharmos. Neste caso,
denotaremos por \(C_{K}^{\infty}\), sendo ele o espaço das funções suaves com suporte - \(\mathrm{supp}(f) = \overline{\{x: f(x)\neq0\}}\) - compacto. Além de um espaço, é importante termos uma noção adequada de convergência e topologia
para trabalharmos e, para isso, iremos definir a convergência no sentido \(C_{K}^{\infty}.\)
\begin{def*}
	Sejam \(f_{j}, f\in C_{K}^{\infty}\). Diremos que \(f_{j}\to f\) no sentido \(C_{K}^{\infty}\) se existe \(K\) compacto tal que
	\begin{itemize}
		\item Para todo j, \(\mathrm{supp}(f_{j})\subseteq K\);
		\item Temos a convergência uniforme \(f_{j}\to f\);
		\item As derivadas de \(f_{j}\) convergem uniformemente para as derivadas de f, i.e., \(D^{m}f_{j}\to Df_{j}\) para todo m. \(\square\)
	\end{itemize}
\end{def*}
Estabelecemos a notação \(D^{k} = f^{(k)}\).
Como não temos certeza se essa noção de convergência transforma \(C_{K}^{\infty}\) em um espaço de Banach, então não ganharíamos muito trabalhando com funcionais lineares limitados. Sendo assim, introduziremos
os \textit{funcionais lineares contínuos}.
\begin{def*}
	Um mapa \(f:X\rightarrow \mathbb{C}\) é um \textbf{funcional linear contínuo} em X se:
	\begin{itemize}
		\item[i)] F é linear, ou seja, \(F(x+y) = Fx + Fy\) e \(F(cx) = cFx\)
		\item[ii)] Sempre que \(f_{j}\to f\) em X, \(F(f_{j})\to F(f)\). \(\square\)
	\end{itemize}
\end{def*}
Com isso, estamos prontos para a definição principal
\begin{def*}
	Uma \textbf{distribuição} é um funcional linear contínuo \(F:C_{K}^{\infty}\rightarrow \mathbb{C}\) com a noção de convergência de \(C_{K}^{\infty}.\) \(\square\)
\end{def*}
\begin{example}
	Se g é uma função contínua, defina
	\[
		G_{g}(f) = \int_{\mathbb{R}}^{}f(x)g(x)dx,\quad f\in C_{K}^{\infty}.
	\]
	Então,
	\begin{align*}
		G_{g}(f + \varphi ) = \int_{\mathbb{R}}^{}(f(x)+\varphi (x))g(x)dx & = \int_{\mathbb{R}}^{}f(x)g(x) + \varphi (x)g(x)dx                       \\
		                                                                   & = \int_{\mathbb{R}}^{}f(x)g(x)dx + \int_{\mathbb{R}}^{}\varphi (x)g(x)dx \\
		                                                                   & = G_{g}(f) + G_{g}(\varphi ).
	\end{align*}
	Além disso, \(G_{g}(cf) = \int_{\mathbb{R}}^{}cf(x)g(x)dx = c \int_{\mathbb{R}}^{}f(x)g(x)dx = cG_{g}(f).\) Finalmente, suponha que \(f_{j}\) converge para f no sentido \(C_{K}^{\infty}.\) Em particular, isto significa que todas a função é suave e todas
	as convergências são uniforme, permitindo a troca do limite da integral com o limite da função, o que fornece
	\begin{align*}
		\lim_{j\to \infty}G_{g}(f_{j}) = \lim_{\to \infty} \int_{\mathbb{R}}^{}f_{j}(x)g(x)dx & = \int_{\mathbb{R}}^{}f_{j}(x)g(x)dx         \\
		                                                                                      & = \int_{\mathbb{R}}^{}f(x)g(x)dx = G_{g}(f).
	\end{align*}
\end{example}
Este exemplo será corriqueiro ao longo do assunto, pois ele fornece a forma de relacionar assuntos das funções contínuas com distribuições. Além disso, saber o valor de \(G_{g}(f)\) para todas as funções \(f\in C_{K}^{\infty}\) caracteriza g
de forma única (a menos de um conjunto de medida nula).
\begin{example}
	Coloque, para \(f\in C_{K}^{\infty},\)
	\[
		\delta (f) = \left\{\begin{array}{ll}
			f(0), & \quad x=0     \\
			0,    & \quad x\neq 0
		\end{array}\right..
	\]
	As propriedades de linearidade seguem facilmente. Quanto à continuidade, suponha que \(f_{j}\to f\) no sentido \(C_{K}^{\infty}.\) Então,
	\[
		\delta (\lim_{j\to \infty}f_{j})  = \left\{\begin{array}{ll}
			\lim_{j\to \infty}f_{j}(0), & x = 0   \\
			0,                          & x\neq 0
		\end{array}\right.  = \left\{\begin{array}{ll}
			f(0), & x = 0   \\
			0,    & x\neq 0
		\end{array}\right. = \delta (f).
	\]
	Esta distribuição é de extrema relevância, levando o nome de \textbf{Delta de Dirac}.
\end{example}
\begin{example}
	Se \(k\geq 1,\) defina \(F(f) = D^{k}f(0)\) para \(f\in C_{K}^{\infty}.\)
\end{example}
Por meio de distribuições conhecidas, podemos obter outras através de diversas operações. Veremos mais exemplos para ilustrar isso, mas a maioria não será elaborada na demonstração de ser uma distribuição.
\begin{example}
	Seja h uma função \(C^{\infty},\) não necessariamente com suporte compacto. Dada uma distribuição F, defina \(M_{h}(f)\) como
	\[
		M_{h}(F)(f) = F(fh),\quad f\in C^{\infty}.
	\]
	Este exemplo mostra que a distribuição do produto de funções \(C^{\infty}\) ainda é uma distribuição. Na verdade, \(M_{h}\) da forma que foi definida funciona como uma extensão do produto de uma função \(C^{0}\), f, com uma função suave (\(C^{\infty}\)), h.
	Em particular, isto significa que este exemplo generaliza o primeiro que foi dado através de
	\[
		M_{h}(G_{g})(f) = G_{g}(fh) = \int_{\mathbb{R}}^{}(fh)gd\mu  = \int_{\mathbb{R}}^{}f(hg)d\mu = G_{hg}(f).
	\]
\end{example}
\begin{example}
	Se F é uma distribuição, defina \(D(F)\) como
	\[
		D(F)(f) = F(-Df),
	\]
	Este exemplo ilustra como podemos obter a derivada de funções contínuas \underline{quaisquer}. Para entender o que isso quer dizer, note que
	\begin{align*}
		D(G_{g})(f) = G_{g}(-Df) = \int_{\mathbb{R}}^{}(-Df)(x)g(x)dx & = - \int_{}^{}(Df)(x)g(x)dx            \\
		                                                              & = \int_{\mathbb{R}}^{}f(x)(Dg)(x)dx    \\
		                                                              & = G_{Dg}(f),\quad f\in C_{K}^{\infty}.
	\end{align*}
	Como mencionado previamente, conhecer \(G_{g}(f)\) para toda f suave de suporte compacto caracteriza, unicamente a menos de um conjunto nulo, a função g. Sendo assim, encontrar
	\(G_{Dg}(f) = D(G_{g})(f) = G_{g}(-Df)\) para toda \(f\in C_{K}^{\infty}\) fornece uma caracterização única para \(Dg\) independente de g ser derivável ou não!.
	Por exemplo, se \(g(x) = |x|,\) então
	\[
		D|x| = \int_{-\infty}^{\infty}|x|f'(x) = \int_{-\infty}^{0}xf'(x)dx - \int_{0}^{\infty}xf'(x)dx = - \int_{-\infty}^{0}f(x)dx + \int_{0}^{\infty}f(x)dx = \int_{\mathbb{R}}^{}\mathrm{sgn(x)}f(x)dx.
	\]
	Em outras palavras, \(|x|' = \mathrm{sgn}(x).\)
\end{example}
\begin{example}
	Dado \(a\in \mathbb{R},\) coloque \(f_{-a}(x)\coloneqq f(x+a)\) e \(Rf(x)\coloneqq f(-x)\). Definimos, então, para \(f\in C_{K}^{\infty},\)
	\begin{align*}
		 & T_{a}(F)(f) = F(f_{-a}) \\
		 & R(F)(f) = F(Rf).
	\end{align*}
	A primeira define uma translação por +a na origem para a distribuição F, enquanto que, a segunda, ilustra uma reflexão dela na origem.
\end{example}
\begin{example}
	As distribuições fornecem, também, uma generalização da convolução por meio de uma função h com suporte compacto. Colocamos
	\[
		C_{h}(F)(f) = F(f*Rh),\quad f\in C_{K}^{\infty}\quad\&\quad Rh(x) = h(-x).
	\]
	Com isto, se \(F = G_{g},\)
	\begin{align*}
		C_{h}(G_{g})(f) = G_{g}(f*Rh) & =\int_{\mathbb{R}}^{}g(x)(f*Rh)(x)dx                   \\
		                              & = \int_{}^{}\biggl(\int_{}^{}g(x)f(y)h(y-x)dy\biggr)dx \\
		                              & = \int_{}^{}\biggl(\int_{}^{}f(y)g(x)h(y-x)dx\biggr)dy \\
		                              & = \int_{}^{}f(y)(g*h)(y)dy = G_{g*h}(f),
	\end{align*}
	ou seja, \(C_{h}\) leva distribuição correspondente à função contínua g na correspondente à função contínua g*h.
\end{example}
Vamos, agora, olhar mais a fundo para um tipo específico de distribuições - aquelas com suporte pontual. Mostraremos também que todas as distribuições com suporte pontual são combinações lineares
de derivadas da função delta (considere a função delta em si como a 0-ésima derivada). Antes, porém, é preciso dizer o que é uma distribuição suportada em um ponto.
\begin{def*}
	Seja G um aberto. Uma distribuição F é \textbf{nula} em G se \(F(f) = 0\) para todas as \(f\in C_{K}^{\infty}\) tais que \(\mathrm{supp}(f)\subseteq G\). \(\square\)
\end{def*}
\begin{lemma*}
	Se F é nula em \(G_{1}\) e em \(G_{2},\) então F é nula em \(G_1\cup G_2\).
\end{lemma*}
\begin{proof*}
	A partir do ponto que f tem suporte em \(G_{1}\cup G_{2},\) prova-se que \(f = f_{1} + f_{2}\) é uma decomposição possível, com \(f_{1}\) suportada em \(G_{1}\) e \(f_{2}\) em \(G_{2},\) de forma que
	\[
		F(f) = F(f_{1} + f_{2}) = F(f_{1}) + F(f_{2}) = 0 + 0 = 0,
	\]
	provando o resultado. \qedsymbol
\end{proof*}
A necessidade de tal lema é que, com ele, dada uma coleção \(\{G_{\alpha }\}\) de abertos tais que F é nula em todo \(G_{\alpha }\) e \(\mathrm{supp}(f)\subseteq \bigcup_{\alpha }^{}G_{\alpha }\), a compacidade fornece uma cobertura finita de
\(\mathrm{supp}(f)\) por \(G_{\alpha }\)'s, isto é, por um número finito de abertos nos quais F é nula. Isto permite que nós apliquemos o lema sem medo de dar errado para concluir que a união de todos os conjuntos nos quais F é nula é, em si, um conjunto
no qual F é nula. Logo, a proposta de definição a seguir está bem-definida:
\begin{def*}
	Se F é uma distribuição, definimos \textbf{o suporte de F} como o complemento do conjunto união de todos os abertos em que F é nula. \(\square\)
\end{def*}
Dita em outros termos, o suporte de uma distribuição é o complementar do maior conjunto no qual ela é nula, i.e.,
\[
	\mathrm{supp}(F) = \biggl(\bigcup_{\alpha \in A}^{}G_{\alpha }\biggr)^{\complement},
\]
em que F é nula em cada \(G_{\alpha }\).
\begin{example}
	Se \(\delta \) é o Delta de Dirac, então \(\mathrm{supp}(\delta ) = \{0\}\). Para ver isto, note que os conjuntos abertos nos quais \(\delta \) é nula são necessariamente todos os subconjuntos abertos de \(\mathbb{C}\) que não contêm a origem. Desta forma, denotando-os por \(G_{\alpha },\)
	\[
		\bigcup_{\alpha \in A}^{}G_{\alpha } = \mathbb{C}\setminus{\{0\}}.
	\]
	Portanto, \(\mathrm{supp}(\delta ) = \biggl(\mathbb{C}\setminus{\{0\}}\biggr)^{\complement} = \{0\}.\)
\end{example}
Nosso próximo passo será definir uma boa norma para obtermos o resultado que queremos. Colocamos
\[
	\Vert f \Vert_{C^{N}(K)} \coloneqq \max_{0\leq k\leq N}\sup_{x\in K}|D^{k}f(x)|.
\]
\begin{prop*}
	Dada uma distribuição F e um compacto K fixo, existem \(N\coloneqq N(f, k)\) e \(c\coloneqq c(F, k)\) tais que, se f é uma função suave com suporte compacto contido em K, então
	\[
		|F(f)|\leq c\Vert f \Vert_{C^{N}(K)}.
	\]
\end{prop*}
\begin{proof*}
	A prova seguirá por contradição - ao assumir que o resultado não vale, mostraremos que F falha em ser uma distribuição. Sendo assim, suponha falso o resultado. Para cad m natural e c real, deve existir ao menos uma \(f_{m}\in C_{K}^{\infty}\) tal que
	\[
		|F(f_{m})| \geq c\Vert f \Vert_{C^{N}(K)}.
	\]
	Sem perda de generalidade, podemos assumir que \(|F(f_{m})| = 1\) para todo m. Se \(f_{m}\to f\), em particular, então \(|F(f)| = 1.\) No entanto,
	\[
		1 > c \Vert f \Vert_{C^{m}(K)}\Rightarrow 1 > c \max_{0 \leq k \leq m}\sup_{x\in X}|D^{k}f(x)|.
	\]
	Com isso, vale, para todo c,
	\[
		\frac{1}{m}\geq \max_{0 \leq k \leq m}\sup_{x\in X}|D^{k}f(x)|,\quad \forall m\in \mathbb{N}.
	\]
	Tomando o limite, obtemos
	\[
		0 = \lim_{m\to \infty}\frac{1}{m} \geq \lim_{m\to \infty}\max_{0\leq k\leq m}\sup_{x\in X}|D^{k}f(x)| \Rightarrow \lim_{m\to \infty}f_{m} = f = 0.
	\]
	Contradição, pois \(F(0) = 0\) e \(F(f) = 1\). \qedsymbol
\end{proof*}
Antes de provar o resultado principal, provaremos outro resultado que será usado.
\begin{prop*}
	Suponha que F seja uma distribuição com \(\mathrm{supp}(F) = \{0\}\). Então, existe N tal que, se \(f\in C_{K}^{\infty}\) cumpre que todas as derivadas antes da N-ésima, quando calculadas em 0, anulam-se, então F é nula em f. Matematicamente, se \(D^{j}f(0) = 0\) para
	\(j\leq N\), então \(F(f) = 0\).
\end{prop*}
\begin{proof*}
	Defina a função
	\[
		\varphi (x) = \left\{\begin{array}{ll}
			0,\quad & x\in [-1, 1] \\
			1,\quad & |x| > 2.
		\end{array}\right.
	\]
	O intuito desta função é ``cortar'' parte da f e obtermos outra função \(g = (1-\varphi )f.\) Note que \(\mathrm{supp}(g) = \overline{\{x: (1-\varphi )f\neq 0\}}.\) Quanto a este conjunto de dentro, ele pode ser reescrito como
	\begin{align*}
		\{x: (1-\varphi )f\neq 0\} & = \{x:(1-\varphi)\neq0\}\cap \{x: f\neq 0\} \\
		                           & = \{x: \varphi \neq 1\}\cap \{x: f\neq 0\}  \\
		                           & = (-2, 2)\cap \{x:f\neq 0\}.
	\end{align*}
	Assim, para ter certeza de que \(\mathrm{supp}(g)\) estará contido em um compacto, escolha \(K = [-3, 3].\) Além disso,
	\[
		F(g) = F((1-\varphi )f) = F(f) - F(\varphi ),
	\]
	mas, para \(x\in [-1, 1], \varphi f = 0\), tal que \(F(\varphi f) = 0\), já que \(F\) tem suporte em \(\{0\}\) (isto significa que todos os valores não-nulos deveriam se concentrar em \(\{0\}\)), tal que, como \(\{0\}\subseteq [-1, 1]\), não há valores não-nulos para \(F(\varphi f).\)
	Em conclusão, \(F(g) = F(f).\) Ademais, \(D^{j}g = [D^{j}f](1-\varphi ) + [D^{j}(1-\varphi )]f = (1-\varphi )D^{j}f,\) de modo que, calculando em 0, obtemos \(D^{j}g(0) = (1-\varphi )D^{j}f(0) = 0\) para todo \(j\leq N\). Com isso, podemos provar o teorema trabalhando com g ao invés de f e chegando em \(F(g) = 0.\)

	Começamos observando que \(|F(g)|\leq c\Vert g \Vert_{C^{N}(K)},\) i.e., ele é limitado. Com base nisso, vamos criar uma sequência cuja função será ``afunilar'' a g até seu valor. Defina, então, \(g_{m}(x)=\varphi (mx)g(x).\) Vejamos dois casos, então.
	1.) Se \(|mx| = m|x| > 2, \) então \(g_{m}(x) = 1 \cdot g(x) = g(x),\) ou seja, para \(3 > |x| >\frac{2}{m}\), a sequência é constante.

	2.) Se \(m|x| \leq 2 \), olharemos com mais atenção. Sabendo o valor de \(D^{j}g(0)\) para todo \(j\leq N\), podemos expandir \(D^{j}g(x)\) em Taylor como segue:
	\[
		D^{j}g(x) = D^{j}g(0) + D^{j+1}g(0)(x-0) + \frac{1}{2!}D^{j+2}g(0)(x-0)^{2} + \dotsc + D^{j+N}g(0)\frac{(x-0)^{N-j}}{(N-j)!} + R = R,
	\]
	em que R é o resto que o próprio teorema fornece, satisfazendo
	\[
		|R|\leq \sup_{y\in \mathbb{R}}|D^{N+1}g(y)|\frac{|x|^{N+1-j}}{(N+1-j)!}.
	\]
	Como \(|x| < \frac{2}{m},\)
	\[
		|R|\leq \biggl[\frac{2^{N+1-j}}{(N+1-j)!}\sup_{y\in \mathbb{R}}|D^{N+1}g(y)|\biggr]m^{j-N-1} = c_{1}m^{j-N-1}.
	\]
	Além disso, de \(g_{m}(x) = \varphi (mx)g(x)\leq g(x)\), chegamos em
	\[
		|g_{m}(x)|\leq c_{2}|g(x)|\leq c_3m^{0 - N - 1} = c_{3}m^{-N-1},
	\]
	em que \(c_2, c_3\) são constante. Aplicando a Regra do Produto e a Regra da Cadeia, temos
	\begin{align*}
		|Dg_{m}(x)| = |[D\varphi (mx)]g(x) + [Dg(x)]\varphi (mx)| & = |mD\varphi(mx)g(x) + \varphi (mx)Dg(x)|                        \\
		                                                          & \leq m|D\varphi (mx)||g(x)|+|\varphi (mx)||Dg(x)|                \\
		                                                          & \leq m \cdot 0 \cdot |g(x)| + 1 \cdot c_4 m^{1-N-1} = c_4m^{-N}.
	\end{align*}
	Com este mesmo raciocínio repetido, obtemos, para \(k\leq N\),
	\[
		|D^{k}g_m(x)| \leq c_5m^{k-1-N}.
	\]
	Pela compacidade de K e por \(g_m(x) = g(x)\) se \(|x|>2/m\), segue a convergência uniforme
	\[
		\lim_{m\to \infty}D^{j}g_{m}(x) = D^{j}g(x),\quad j\leq N.
	\]
	No entanto, \(\lim_{m\to \infty}|D^{j}g_{m}(x)|\leq \lim_{m\to \infty}\frac{c_1}{m^{N+1-j}} = 0\), ou seja, \(g_m\) é 0 em uma vizinhança de 0. Consequentemente, \(F(g_m) = 0\). Portanto,
	\[
		0 = \lim_{m\to \infty}F(g-g_m) = \lim_{m\to \infty}F(g) - \lim_{m\to \infty}F(g_m) = F(g) - 0 = F(g).\quad \text{\qedsymbol}
	\]
\end{proof*}
Com esta proposição, encontramos uma propriedade presente em todas as distribuições com suporte \(\{0\}.\) Utilizando o exemplo da derivada da distribuição, observe que \(D^{j}\delta (f) = (-1)^{j}D^{j}(f(0)).\)
Finalmente, então, podemos ver o último resultado.
\begin{theorem*}
	Suponha que F é uma distribuição com suporte em \(\{0\}.\) Então, existem N e constantes \(c_{i}\) tais que
	\[
		F = \sum\limits_{i=0}^{N}c_{i}D^{i}\delta .
	\]
\end{theorem*}
\begin{proof*}
	Seja \(P_{i}(x)\) uma função \(C_{K}^{\infty}\) que coincide com \(x^{i}\) perto de 0, tal que \(D^{j}P_{i}(0) = j!x^{i-j}\biggl|_{0}^{}\biggr. =0\) para \(i\neq j\) e \(i!\) se \(i= j\). Assim,
	\(D^{j}\delta (P_{i}) = \delta (-D^{j}P_{i}) = (-1)^{j}D^{j}P_{i}(0) = \frac{(-1)^{i}}{i!}\) se \(i = j\). Caso contrário, \(D^{j}\delta (P_{i}) = 0.\)

	Pela proposição anterior, podemos determinar o N. Começando por \(f\in C_{K}^{\infty}\) e aplicando Taylor,
	\[
		g(x) = \sum\limits_{i=0}^{N}\frac{D^{i}f(0)}{i!}P_{i}(x)
	\]
	coincide com f em 0, assim como suas N primeiras derivadas. Pela proposição,
	\[
		F \biggl(f - \sum\limits_{i=0}^{N}\frac{0^{i}f(0)}{i!}P_{i}\biggr) = 0 \Rightarrow F(f) = \sum\limits_{i=0}^{N}\frac{D^{i}f(0)}{i!}F(P_{i}) = \sum\limits_{i=0}^{N} \frac{(-1)^{i}}{i!}D^{i}\delta (f)F(P_{i}).
	\]
	Portanto, fazendo \(c_{i} = \frac{(-1)^{i}}{i!}F(P_{i}),\) o teorema está provado, já que f era arbitrária e \(c_{i}\) não dependem de f. \qedsymbol
\end{proof*}

\subsection{Kalel B. - Teoria Espectral}
Neste seminário, veremos alguns resultados comuns para a Teoria Espectral, incluindo operadores lineares, operadores adjuntos/Hermitianos, simétricos, compactos e como a Teoria da Medida fornece ferramentas
capazes de melhorar o estudo desta parte. Sem mais delongas, um lembrete do que são autovalores e autovetores. Dada uma operação linear \(A\), um \textbf{autovalor} \(\lambda \) de A é um escalar que satisfaz \(Av = \lambda v\).
Se existir tal \(\lambda \), então v é chamado de \textbf{autovetor}. Equivalentemente, um escalar \(\lambda \) é um autovalor se ele satisfaz
\[
	\det{(\lambda I - A)} = 0.
\]
\begin{def*}
	Seja H um espaço de Hilbert de números complexos. Um operador linear \(A:H\rightarrow H \) é \textbf{limitado} se
	\[
		\Vert A \Vert = \sup_{}\{\Vert Ax \Vert: \Vert x \Vert \leq 1\} < \infty.\quad \square
	\]
\end{def*}
\begin{prop*}
	Dados dois operadores lineares limitados \(A, B:H\rightarrow H\) e \(c\in \mathbb{C},\) temos
	\begin{itemize}
		\item[1)](A+B)(x) = Ax + Bx;
		\item[2)] (cA)(x) = cA(x).
	\end{itemize}
\end{prop*}
\begin{prop*}
	Seja \(\mathcal{L}\) o conjunto de todos os operadores lineares limitados de H. \(\mathcal{L}\) é um espaço vetorial e um espaço de Banach com a norma \(\Vert A \Vert = \sup_{\Vert x \Vert = 1}\{\Vert Ax \Vert\}.\)
\end{prop*}
\begin{def*}
	A \textbf{composição de operadores} é definida como
	\[
		(AB)(x) = A(Bx).\quad \square
	\]
\end{def*}
Essa composição não é comutativa. Para ver isto, basta tomar A e B como matrizes quadradas e a composição como a multiplicação usual de matrizes, que não é comutativa.
Além disso, dados dois operadores limitados A e B, temos
\[
	\Vert AB \Vert \leq \Vert A \Vert\Vert B \Vert
\]
e, em particular,
\[
	\Vert A^{n} \Vert\leq \Vert A \Vert^{n}.
\]
\begin{def*}
	Sejam \(A:H\rightarrow H\) um operador linear limitado e \(\lambda \in \mathbb{C}\)
	\begin{itemize}
		\item O \textbf{espectro} de A é
		      \[
			      \sigma (A) = \{\lambda \in \mathbb{C}: A - \lambda I \text{ não é inversível}\}.
		      \]
		\item O \textbf{resolvente} de A é o conjunto dos números complexos para os quais \(A - \lambda I\) é inversível, \textit{i.e.},
		      \[
			      R_{\lambda }(A) = \mathbb{C}\setminus{\sigma (A)}.
		      \]
		\item O \textbf{raio espectral} de A é definido por
		      \[
			      r(A) = \sup_{}\{|\lambda |: \lambda \in \sigma (A)\}.
		      \]
	\end{itemize}
\end{def*}
Vale observar que nem todo elemento do espectro é um autovalor.
\begin{example}
	Seja \(H = l^{2},\) o qual é um espaço de Hilbert com a norma
	\[
		\Vert x \Vert = \biggl(\sum\limits_{n=1}^{\infty}|x_{n}|^{2}\biggr)^{\frac{1}{2}}\quad\&\quad x=(x_{n})\in l^{2} \text{ se }\sum\limits_{n=1}^{\infty}|x_{n}|^{2} < \infty.
	\]
	Defina \(A:H\rightarrow H\) por
	\[
		A(a_1, a_2, a_3, \dotsc ) = (a_1, a_2/2, a_3/3, \dotsc ),
	\]
	o qual é limitado, já que
	\[
		\Vert A(a_1, a_2, a_3, \dotsc ) \Vert^{2} = \biggl\Vert \biggl(a_1, \frac{a_2}{2}, \frac{a_3}{3}, \dotsc \biggr)\biggr\Vert^{2} = \biggl(\sum\limits_{n=1}^{\infty}\biggl(\frac{|a_{n}|}{|n|}\biggr)^{2}\biggr) \leq \sum\limits_{n=1}^{\infty}|a_{n}|^{2} = \Vert a_{n} \Vert^{2} < \infty.
	\]
	Este operador A não admite inversa. Para ver isto, seja \(e_{n}\) um elemento de \(\ell^{2}\) tal que
	\[
		e_{n} = (0, 0, \dotsc , 0, 1, 0, \dotsc ),
	\]
	em que o valor 1 é assumido na n-ésima entrada. Sendo assim, se A tivesse inversa, então
	\[
		A^{-1}e_{n} = ne_{n}\not\in \ell^{2},
	\]
	o que faria com que A não fosse sobrejetora e \(A - 0I\) não admitisse inversa, ou seja, 0 estaria no espectro de A. No entanto, não existe \(v\neq 0\) tal que \(Av = 0v\), então 0 não é autovalor.
\end{example}
\begin{prop*}
	Se \(B:H\rightarrow H\) é um operador linear limitado com \(\Vert B \Vert < 1\), então \(I - B\) é inversível e
	\[
		(I-B)^{-1} = \sum\limits_{i=0}^{\infty}B^{i}.
	\]
	Aqui, colocamos a convenção \(B^{0} = I\).
\end{prop*}
\begin{proof*}
	Temos \(\Vert B^{i} \Vert \leq \Vert B \Vert^{i}\), já que \(\Vert B \Vert < 1\). Assim, segue que
	\[
		\sum\limits_{i=0}^{\infty}\Vert B \Vert^{i} < \infty,
	\]
	pois é uma série geométrica com \(\Vert B \Vert < 1.\) Agora, seja \(S_{n} = \sum\limits_{i=0}^{n}B^{i}\). Para \(n\geq m\), temos
	\[
		\Vert S_{m} - S_{n} \Vert = \biggl\Vert \sum\limits_{i=0}^{n}B^{i} - \sum\limits_{i=0}^{m}B^{i}\biggr\Vert = \biggl\Vert \sum\limits_{i=m}^{n}B^{i}\biggr\Vert \leq \sum\limits_{i=m}^{n}\Vert B \Vert^{i}.
	\]
	Logo, para \(m, n > N_{0}\), temos \(\Vert S_{m} - S_{n} \Vert\to 0\), do que segue que \(S_{n}\) é uma sequência de Cauchy de operadores limitados. Aqui, utilizando a completude
	de \(\mathcal{L}\), existe um S para o qual \(S_{n}\) converge, tal que
	\[
		\lim_{n\to \infty}S_{n} = \sum\limits_{i=0}^{\infty}B^{i} = S.
	\]
	Deste modo, \(BS = \sum\limits_{i=1}^{\infty}B^{i} = S - I\), ou seja,
	\[
		(I-B)S = I = S(I-B).
	\]
	Portanto,
	\[
		S = (B-I)^{-1} = \sum\limits_{i=0}^{\infty}B^{i}.\quad \text{\qedsymbol}
	\]
\end{proof*}
\begin{prop*}
	Se \(A:H\rightarrow H\) é um operador linear limitado inversível com \(\Vert B \Vert < \frac{1}{\Vert A^{-1} \Vert}\), então \(A - B\) é invertível.
\end{prop*}
\begin{proof*}
	Temos
	\[
		\Vert A^{-1}B \Vert \leq \Vert A^{-1} \Vert \Vert B \Vert < 1.
	\]
	Então, existe \((I-A^{-1}B)^{-1}\).
	\textbf{\underline{Afirmação}:} Se M e N são inversíveis, então
	\[
		(N^{-1}M^{-1})(MN) = I = (MN)(N^{-1}M^{-1}).
	\]
	Deste modo, considere \(M = A\) e \(N = I - A^{-1}B\), do que segue que
	\begin{align*}
		 & I = ((I-A^{-1}B)^{-1}A^{-1})(A(I-A^{-1}B)) = (A-A^{-1}BA)^{-1}(A-B)) \\
		 & I = A(I-A^{-1}B)((I-A^{-1}B)^{-1}A^{-1}) = (A-B)(A-A^{-1}BA)^{-1}.
	\end{align*}
	Portanto, \(A- B\) é inversível. \qedsymbol
\end{proof*}
\begin{prop*}
	Se \(A:H\rightarrow H\) é um operador linear limitado, então \(\sigma (A)\subseteq \mathbb{C}\) é fechado, limitado e
	\[
		r(A) \leq \Vert A \Vert.
	\]
\end{prop*}
\begin{proof*}
	Seja \(\lambda \not\in \sigma (A)\), ou seja, \(\lambda I - A\) é inversível e considere \(B(\lambda , \delta ), \delta > 0\) suficientemente pequeno. Dado \(\omega \in B(\lambda , \delta )\), temos
	\[
		\omega I - A = \omega I - \lambda I + \lambda I - A = (\lambda I - A) - (\lambda -\omega )I.
	\]
	Como \(\lambda I - A \) é um operador linear limitado, inversível e que satisfaz
	\[
		\Vert (\lambda - \omega )I \Vert = |\lambda  - \omega | < \delta  < \frac{1}{\Vert (\lambda I - A)^{-1} \Vert},
	\]
	segue que \(\omega I - A\) tem inversa. Consequentemente, \(\omega \not\in \sigma (A)\), então \(B(\lambda , \delta )\subseteq \sigma (A)^{\complement}\).
	Disto, obtemos que \(\sigma (A)\) é fechado. Ademais, sabemos que
	\[
		(\lambda I - A)^{-1} = \lambda ^{-1}(I-A\lambda ^{-1})^{-1} = \sum\limits_{n=0}^{\infty}A^{n}\lambda ^{-n-1}
	\]
	é convergente quando \(\Vert A\lambda ^{-1} \Vert < 1\), tal que
	\[
		\Vert A\lambda ^{-1} \Vert = |\lambda ^{-1}| \Vert A \Vert < 1 \Longleftrightarrow \Vert A \Vert < |\lambda |,
	\]
	de modo que, se \(|\lambda | > \Vert A \Vert\), temos \(\lambda I - A\) inversível e, a partir disto, \(\lambda \not\in \sigma (A)\).

	Portanto, dado \(\lambda \in \sigma (A)\), temos \(|\lambda |\leq \Vert A \Vert\), tal que \(r(A) \leq \Vert A \Vert\) e \(\sigma (A) \subseteq B[0, \Vert A \Vert]\). \qedsymbol
\end{proof*}
\begin{def*}
	Se \(A:H\rightarrow H\) é um operador linear limitado, o \textbf{adjunto} de A, denotado por \(A^{*},\) é um operador tal que
	\[
		\left< Ax, y \right> = \left<  x, A^{*}y \right>,\quad \forall x, y\in H.\quad \square
	\]
\end{def*}
Observe que, desta definição, segue que
\[
	(cA)^{*} = \overline{c}A^{*},\quad (A^{n})^{*} = (A^{*})^{n},\quad\&\quad \overline{P(A)} = \sum\limits_{j=0}^{n}\overline{a}_{j}P(A^{*}) = \overline{P}(A^{*}),
\]
em que \(P(A) = \sum\limits_{j=0}^{n}a_{j}A^{j}\).

\begin{prop*}
	Se \(A:H\rightarrow H\) é um operador limitado, então existe um único operador \(A^{*}\) tal que \(\left< Ax, y \right> = \left< x, A^{*}y \right>\) para todo x e y.
\end{prop*}
\begin{proof*}
	Fixe y. Defina \(f(x) = \left< Ax, y \right>,\) o qual é um funcional linear em H que satisfaz
	\[
		|f(x)| = |\left< Ax, y \right>|\leq \Vert Ax \Vert\Vert y \Vert \leq \Vert A \Vert\Vert y \Vert.
	\]
	Logo, f é limitado e segue do \hyperlink{riesz_representation}{\textit{Teorema da Representação de Riesz}} que existe um único \(z_{y}\) que satisfaz a igualdade
	\[
		\left< Ax, y \right> = f(x) = \left< x, z_{y} \right>
	\]
	para todo x. Isto implica que \(y = z_{y}\). Ademais,
	\[
		\left< x, z_{y_1 + y_2} \right> = \left< Ax, y_1 + y_2 \right> = \left< Ax, y_1 \right> + \left< Ax, y_2 \right> = \left< x, z_{y_1} \right> + \left< x, z_{y_2} \right> = \left< x, z_{y_1} + z_{y_2} \right>
	\]
	para todo x. Com isso, \(z_{y_1 + y_2} = z_{y_{1}} + z_{y_2}.\).

	Do mesmo modo, isso vale para o produto de y por um escalar também, \textit{i.e.}, \(z_{cy} = cz_{y}.\) Assim, basta definirmos \(A^{*}y = z_{y},\) pois
	\[
		\left< Ax, y \right> = \left< x, z_{y} \right> = \left< x, A^{*}y \right>.
	\]
	Resta, então, apenas provarmos a unicidade. Para isso, considere \(A_1\) e \(A_2\) dois operadores tais que
	\[
		\left< x, A_1y \right> = \left< Ax, y \right> = \left< x, A_2 y \right>
	\]
	para todo x e y. Destarte, \(A_1 y = A_2y\) para todo y. Portanto, \(A_1 = A_2\). \qedsymbol
\end{proof*}
\begin{def*}
	Seja \(A:H\rightarrow H\) um operador linear limitado. A é dito \textbf{simétrico}, ou \textbf{Hermitiano}, ou ainda \textbf{auto-adjunto} se
	\[
		\left< Ax, y \right> = \left< x, Ay \right>
	\]
	para todo x e y em H. \(\square\)
\end{def*}
\begin{example}
	Sejam \((X, \mathcal{A}, \mu )\) um espaço de medida com \(\mu \) medida \(\sigma \)-finita e \(H = L^{2}(X)\), o qual é um espaço de Hilbert. Seja \(F:X\times X\rightarrow \mathbb{C}\) uma função mensurável tal que \(F(y, x) = \overline{F(x, y)}\) e
	\[
		\int_{}^{}\int_{}^{}F(x, y)^{2}\mu (dx)\mu (dy) < \infty.
	\]
	Defina \(A:H\rightarrow H\) por
	\[
		Af(x) = \int_{}^{}F(x, y)f(y)\mu (dy).
	\]
	Mostraremos que A é limitado e simétrico. Com efeito, note que
	\[
		\biggl\vert \int_{}^{}F(x, y)f(y)\mu (dy) \biggr\vert \leq \int_{}^{}|F(x, y)f(y)|\mu (dy)
	\]
	pela \hyperlink{holder}{\textit{Desigualdade de Hölder}} (p=q=2),
	\[
		\int_{}^{}|F(x, y)f(y)|\mu (dy) \leq \biggl(\int_{}^{}|F(x, y)|^{2}\mu (dy)\biggr)^{\frac{1}{2}}\biggl(\int_{}^{}|f(y)|^{2}\mu (dy)\biggr)^{\frac{1}{2}}.
	\]
	Logo,
	\[
		\biggl\vert \int_{}^{}F(x, y)f(y)\mu (dy) \biggr\vert^{2}\leq \biggl(\int_{}^{}|F(x, y)|^{2}\mu (dy)\biggr)\biggl(\int_{}^{}|f(y)|^{2}\mu (dy)\biggr).
	\]
	Integrando isto, chegamos em
	\[
		\int_{}^{}\biggl\vert \int_{}^{}F(x, y)f(y)\mu (dy) \biggr\vert^{2}\mu (dx) \leq \int_{}^{}\biggl(\int_{}^{}|F(x, y)|^{2}\mu (dy)\biggr)\mu (dx)\biggl(\int_{}^{}|f(y)|^{2}\mu (dy)\biggr),
	\]
	já que o último termo à direita não depende de x. Por hipótese e \(f\in L^{2},\) temos
	\[
		\Vert Af(x) \Vert_{2}^{2} = \int_{}^{}\biggl\vert \int_{}^{}F(x, y)f(y)\mu (dy) \biggr\vert^{2}\mu (dx) < \infty.
	\]
	Sendo assim, na norma do operador, vamos ter que \(\Vert Af \Vert\) é finito. Logo, A é limitado.

	Agora, vamos mostrar que A é simétrico. Sejam \(f, g\in L^{2}(X)\). Temos
	\begin{align*}
		\left< Af, g \right> = \int_{}^{}Af \overline{g}d\mu  = \int_{}^{}Af(x)\overline{g}(x)\mu (dx) & = \int \int_{}F(x, y)f(y)\mu (dy)\overline{g}(x)\mu (dx).
		                                                                                               & = \int_{}^{}\int_{}^{}F(x, y)f(y)\overline{g}(x)\mu (dy)\mu (dx)            \\
		                                                                                               & = \int_{}^{}\int_{}^{}F(x,y)\overline{g}(x)f(y)\mu (dx)\mu (dy)             \\
		                                                                                               & = \int_{}^{}\overline{\int_{}^{}\overline{F(x, y)}g(x)}\mu (dx)f(y)\mu (dy) \\
		                                                                                               & = \int_{}^{}\overline{\int_{}^{}F(y,x)g(x)}\mu (dx)f(y)\mu (dy)             \\
		                                                                                               & = \int_{}^{}f(y)\overline{A}g(y)\mu (dy) = \left< f, Ag \right>.
	\end{align*}
	Portanto, \(A = A^{*}\).
\end{example}
\begin{prop*}
	Seja \(A:H\rightarrow H\) um operador limitado simétrico.
	\begin{itemize}
		\item[1)] \(\left< Ax, x \right>\) é real para todo x em H;
		\item[2)] A função \(x\mapsto \left< Ax, x \right>\) é nula se, e somente se, \(a = 0\);
		\item[3)] Vale que \(\Vert A \Vert = \sup_{\Vert x \Vert = 1}|\left< Ax, x \right>|\).
	\end{itemize}
\end{prop*}
\begin{proof*}
	1): Temos
	\[
		\left< Ax, x \right> = \left< x, Ax \right> = \overline{\left< Ax, x \right>}.
	\]
	Logo, \(\left< Ax, x \right>\in \mathbb{R}.\)

	2): Se \(A = 0\), então \(\left< Ax, x \right> = 0.\) Agora, se \(\left< Ax, x \right> = 0\) para todo x, segue que
	\begin{align*}
		0 = \left< A(x+y), x+y \right> & = \left< Ax, x \right> + \left< Ax, y \right> + \left< Ay, x \right> + \left< Ay, y \right> \\
		                               & = \left< Ax, y \right> + \left< Ay, x \right>                                               \\
		                               & = \left< Ax, y \right> + \left< y, Ax \right>                                               \\
		                               & = \left< Ax, y \right> + \overline{\left< Ax, y \right>}.
	\end{align*}
	Logo, \(\mathrm{Re}\left< Ax, y \right> = 0\). Analogamente, trocando x por ix e sabendo que \(\mathrm{Im}(z) = -\mathrm{Re}(iz),\) obtemos
	\[
		\mathrm{Im}(\left< Ax, y \right>) = -\mathrm{Re}(i \left< Ax, y \right>) = - \mathrm{Re}(\left< Aix, y \right>) = 0.
	\]
	Disto, chegamos em \(\left< Ax, y \right> = 0\) para todo x e y, ou seja, \(Ax = 0\) para todo x. Logo, A = 0.

	3): Finalmente, seja \(\beta = \sup_{\Vert x \Vert = 1}|\left< Ax, x \right>|\). Pela \hyperlink{cauchy_schwarz}{\textit{Desigualdade de Cauchy-Schwarz}}, temos
	\[
		|\left< Ax, x \right>|\leq \Vert Ax \Vert\Vert x \Vert \leq \Vert A \Vert\Vert x \Vert^{2},
	\]
	tal que \(\beta \leq \Vert A \Vert\). Agora, sejam \(\Vert x \Vert = 1\) e \(y\in H\) tais que \(\left< y, Ax \right>\) é real e \(\Vert y \Vert = 1.\) Vale que
	\[
		\left< y, Ax \right> = \frac{1}{4}(\left< x + y, A(x, y) \right> - \left< x - y, A(x-y) \right>,
	\]
	no qual utilizamos que \(\left< y, Ax \right> = \left< Ax, y \right> = \left< x, Ay \right>,\) pois \(\left< y, Ax \right>\) é real e A é simétrico. Com isso, pela \textit{Lei do Paralelogramo},
	\begin{align*}
		16|\left< y, Ax \right>|^{2} & \leq \beta ^{2}(\Vert x + y \Vert^{2} + \Vert x-y \Vert^{2})^{2} \\
		                             & = 4\beta ^{2}(\Vert x \Vert^{2} + \Vert y \Vert^{2})^{2}         \\
		                             & = 16\beta ^{2}.
	\end{align*}
	Com isto, \(|\left< y, Ax \right>|\leq \beta \). Caso \(\Vert y \Vert = 1\), mas \(\left< y, Ax \right> = re^{i\theta },\) seja \(y'= e^{-i\theta }\) e prossiga do mesmo jeito que acima, mas com y' no lugar de y. Fazendo isto,
	\[
		|\left< y', Ax \right>| = |\left< y, Ax \right>| \leq \beta ,
	\]
	em que colocamos \(y = \frac{Ax}{\Vert Ax \Vert}.\) Assim, segue que \(\Vert Ax \Vert \leq \beta \) e \(\Vert A \Vert \leq \beta \). Portanto, \(\Vert A \Vert = \beta \). \qedsymbol
\end{proof*}
\begin{prop*}
	Seja \(A:H\rightarrow H\) um operador linear limitado e simétrico. Então, os autovalores de A são reais e, além disso, dois autovetores correspondentes a autovalores distintos são ortogonais.
\end{prop*}
\begin{proof*}
	Note que, se \(\lambda \in \mathbb{C}\) é um autovalor de A, então
	\[
		\lambda \left< v, v \right> = \left< \lambda v, v \right> = \left< Av, v \right> = \left< v, Av \right> = \left< v, \lambda v \right> = \overline{\lambda }\left< v, v \right>,
	\]
	do que segue que \(\lambda  = \overline{\lambda } \) e, consequentemente, \(\lambda \in bm  R\).

	Para ver a ortogonalidade, sejam \(v_1\) e \(v_2\) autovetores correspondentes aos autovalores \(\lambda _1\) e \(\lambda _2\) respectivamente, com \(\lambda _1\neq \lambda _2\). Temos
	\[
		\lambda _1 \left< v_1, v_2 \right> = \left< \lambda_1 v_1, v_2 \right> = \left< Av_1, v_2 \right> = \left< v_1, Av_2 \right> = \left< v_1, \lambda _2v_2 \right> = \overline{\lambda }_2\left< v_1, v_2 \right>,
	\]
	mas \(\lambda_2 \in \mathbb{R}\). Portanto,
	\[
		\lambda _1\left< v_1, v_2 \right>=\lambda _2 \left< v_1, v_2 \right> \Rightarrow \left< v_1, v_2 \right> = 0. \quad \text{\qedsymbol}
	\]
\end{proof*}
\begin{def*}
	Um operador linear \(A:H\rightarrow H\) é dito \textbf{compacto} se \(\overline{A(B)}\) é compacto em H, em que B é a bola aberta unitária. \(\square\)
\end{def*}
\begin{theorem*}
	Seja \(A:H\rightarrow H\) um operador compacto e simétrico. Então, H admite uma base ortonormal formada por autovetores de A. Além disso, existem sequências de autovalores \((\lambda_{n})\) de A e vetores \((v_{n})\) tais que
	cada \(v_{n}\) é autovetor associado a \(\lambda_{n}\) e
	\[
		Ax = \sum\limits_{n}^{}\lambda_{n} \left< x, v_{n} \right>v_{n}.
	\]
\end{theorem*}
\end{document}

 \documentclass[../functional_analysis.tex]{subfiles}
\begin{document}
\section{Aula 06 - 21 de Agosto, 2025}
\subsection{Motivações}
\begin{itemize}
	\item Complexificação;
	\item Espaços de Produto Interno;
\end{itemize}
\subsection{Complexificação}
\begin{def*}
	Seja X um espaço vetorial sobre \(\mathbb{R}.\) A \textbf{complexificação de X} estende X a um espaço vetorial \(X^{\#}\) sobre \(\mathbb{C}\) e é feita como:
	\[
		X^{\#} = \{(x, y):\; x, y\in X\}
	\]
	com a operação de adição feita coordenada a coordenada, e com a operação de multiplicação por escalar dada por
	\[
		(\alpha +i\beta )(x, y) = (\alpha x - \beta y,\; \beta x + \alpha y).\; \square
	\]
\end{def*}
\begin{exr}
	Mostre que a complexificação de um espaço vetorial sobre \(\mathbb{R}\) define um espaço vetorial sobre \(\mathbb{C}.\)
\end{exr}
Note que, com estas operações de multiplicação por escalar, temos
\[
	(x, y) = (x,0)+(0,y)=(x,0)+i(y,0) = ``x+iy'',
\]
tornando natural a identificação de X com \(\{(x, y)\in X^{\#}:\; y =0\}\), que consiste exatamente dos elementos \(x+i*0 = x\in X\).

Se \(\{x_{1},\dotsc , x_{n}\}\) é um conjunto linearmente independente de vetores em X, então \(\{(x_1, 0), \dotsc , (x_{n}, 0)\}\) é um conjunto linearmente independente de vetores em \(X^{\#}\); de fato, supondo que
\[
	\sum\limits_{i=1}^{n}(\alpha_{i}+i\beta_{i})(x_{i}, 0) = \biggl(\sum\limits_{i=1}^{n}(x_{i}, 0), \sum\limits_{i=1}^{n}\beta_{i}(0, x_{i})\biggr) = (0, 0),
\]
então \(\alpha_{i} = \beta_{i} = 0,\) onde \(1\leq i\leq n.\) Neste sentido, também podemos passar bases \(\{x_1, \dotsc , x_{n}\}\) de X para bases \(\{(x_1, 0), \dotsc , (x_{n}, 0)\}\) de \(X^{\#}\).

Com estas construções \textit{básicas}\footnote{Logo depois de definir base.}, segue que se \(A:X\rightarrow X\) é um operador linear, podemos naturalmente estendê-lo a um operador
\begin{align*}
	A^{\#}: & X^{\#}\rightarrow X^{\#}                          \\
	        & (x, y)\longmapsto A^{\#}(x, y)\coloneqq (Ax, Ay),
\end{align*}
sendo que \(A^{\#}\) é de fato uma transformação linear pois, para \((\alpha +i\beta) \in \mathbb{C}\) qualquer,
\begin{align*}
	A^{\#}((\alpha +i\beta )(x, y)) & =(A(\alpha x-\beta y), A(\beta x+\alpha y))    \\
	                                & = (\alpha Ax - \beta Ay, \beta Ax + \alpha Ay) \\
	                                & = (\alpha +i\beta )(Ax, Ay)                    \\
	                                & = (\alpha +i\beta )A^{\#}(x, y),
\end{align*}
e, dados elementos \((x, y), (u, v)\in X^{\#}\),
\begin{align*}
	A^{\#}((x, y) + (u, v)) & = A^{\#}(x+u, y+v)             \\
	                        & = (A(x+u), A(y+v))             \\
	                        & = (Ax + Au, Ay + Av)           \\
	                        & = (Ax, Ay) + (Au, Av)          \\
	                        & = A^{\#}(x, y) + A^{\#}(u, v).
\end{align*}

\begin{def*}
	Considere um espaço vetorial X sobre o corpo \(\mathbb{K}\), uma transformação linear \(A:X\rightarrow X\) e, para cada escalar \(\lambda \in \mathbb{K}\), a transformação linear \(\lambda I - A\); se \(\lambda I - A\) não é injetiva, então existe um vetor x diferente da origem em X tal que
	\[
		(\lambda I-A)x = 0,
	\]
	caso no qual diremos que \(\lambda \) é um \textbf{autovalor de A} e que x é um \textbf{autovetor de A associado ao autovalor }\(\lambda \). \(\square\)
\end{def*}

Se X é um espaço vetorial sobre \(\mathbb{R},\; A:X\rightarrow X\) é uma transformação linear, \(X^{\#}\) a complexificação de X e \(A^{\#}:X^{\#}\rightarrow X^{\#}\) a correspondente complexificação de A, então os autovalores de A permanecem autovalores de \(A^{\#}\), assim como os autovalores reais de \(A^{\#}\) serão autovalores de A: se \(\alpha +i\beta \) é um autovalor de \(A^{\#}\) com autovetor \((x, y)\), então \(\alpha -i\beta \) é um autovalor de \(A^{\#}\) com autovetor \((x, -y)\).

Se \(A:\mathbb{R}^{2}\rightarrow \mathbb{R}^{2}\) é dado por
\[
	A \begin{bmatrix}
		x \\
		y
	\end{bmatrix} = \begin{bmatrix}
		-y \\
		x
	\end{bmatrix},
\]
segue que, para \(\lambda \in \mathbb{R},\) a relação
\[
	(\lambda I - A)\begin{bmatrix}
		x \\
		y
	\end{bmatrix} = \begin{bmatrix}
		0 \\
		0
	\end{bmatrix}
\]
nos dá
\[
	\begin{bmatrix}
		\lambda x - y \\
		\lambda y + x
	\end{bmatrix} = \begin{bmatrix}
		0 \\
		0
	\end{bmatrix},
\]
ou seja, \(x=y=0.\) Logo, nenhum número real pode ser autovalor de A, tal que não existem soluções não nulas do tipo
\[
	e^{\lambda t}\begin{bmatrix}
		x \\
		y
	\end{bmatrix}
\]
para a equação
\[
	\begin{bmatrix}
		x \\
		y
	\end{bmatrix} = A\begin{bmatrix}
		x \\
		y
	\end{bmatrix}.
\]

Agora, com respeito à complexificação de \(\mathbb{R}^{2}\) e \(\tilde{A}\) de A, temos
\begin{align*}
	((\alpha +i\beta )I - \tilde{A}) \biggl( \begin{bmatrix}
			                                         x_1 \\
			                                         y_1
		                                         \end{bmatrix}, \begin{bmatrix}
			                                                        x_2 \\
			                                                        y_2
		                                                        \end{bmatrix}\biggr) & = (\alpha +i\beta ) \biggl(\begin{bmatrix}
			                                                                                                          x_1 \\
			                                                                                                          y_1
		                                                                                                          \end{bmatrix}, \begin{bmatrix}
			                                                                                                                         x_2 \\
			                                                                                                                         y_2
		                                                                                                                         \end{bmatrix}\biggr) - \biggl(A \begin{bmatrix}
			                                                                                                                                                         x_1 \\
			                                                                                                                                                         y_1
		                                                                                                                                                         \end{bmatrix}, A\begin{bmatrix}
			                                                                                                                                                                         x_2 \\
			                                                                                                                                                                         y_2
		                                                                                                                                                                         \end{bmatrix}\biggr)                     \\
	                                                         & = \biggl( \alpha \begin{bmatrix}
			                                                                            x_1 \\
			                                                                            y_1
		                                                                            \end{bmatrix} - \beta \begin{bmatrix}
			                                                                                                  x_2 \\
			                                                                                                  y_2
		                                                                                                  \end{bmatrix}, \beta \begin{bmatrix}
			                                                                                                                       x_1 \\
			                                                                                                                       y_1
		                                                                                                                       \end{bmatrix} + \alpha \begin{bmatrix}
			                                                                                                                                              x_2 \\ y_2
		                                                                                                                                              \end{bmatrix}\biggr) - \biggl(A\begin{bmatrix}
			                                                                                                                                                                             x_1 \\
			                                                                                                                                                                             y_1
		                                                                                                                                                                             \end{bmatrix}, A\begin{bmatrix}
			                                                                                                                                                                                             x_2 \\
			                                                                                                                                                                                             y_2
		                                                                                                                                                                                             \end{bmatrix}\biggr) \\
	                                                         & = \begin{bmatrix}
		                                                             0 \\
		                                                             0
	                                                             \end{bmatrix},
\end{align*}
donde concluímos que
\[
	(\alpha -A)\begin{bmatrix}
		x_1 \\
		y_1
	\end{bmatrix} = \beta \begin{bmatrix}
		x_2 \\
		y_2
	\end{bmatrix} \quad\&\quad (\alpha -A)\begin{bmatrix}
		x_2 \\
		y_2
	\end{bmatrix} = -\beta \begin{bmatrix}
		x_1 \\
		y_1
	\end{bmatrix}.
\]
Disto segue que, como \(A^{2} = -I,\)
\[
	(\alpha^{2}+\beta^{2}-1-2\alpha A)\begin{bmatrix}
		x_1 \\
		y_1
	\end{bmatrix} = \begin{bmatrix}
		0 \\
		0
	\end{bmatrix}.
\]
Logo, existe \(\begin{bmatrix}
	x_1 \\
	y_1
\end{bmatrix}\neq \begin{bmatrix}
	0 \\
	0
\end{bmatrix}\) que satisfaz a equação acima se, e somente se \(\alpha = 0\) e \(\beta =\pm 1\).

Agora, para \(\alpha +i\beta  = \pm i\), os vetores da forma \(\biggl(\begin{bmatrix}
		x_1 \\
		y_1
	\end{bmatrix}, \pm \begin{bmatrix}
		y_1 \\
		-x_1
	\end{bmatrix}\biggr)\) são autovetores de \(\tilde{A}\) associados. Assim, \(x^{\pm} = e^{\pm it} \biggl(\begin{bmatrix}
		x_1 \\
		y_1
	\end{bmatrix}, \pm \begin{bmatrix}
		y_1 \\
		-x_1
	\end{bmatrix}\biggr)\) são soluções de
\[
	\dot{x} = \tilde{A}x
\]
e
\[
	x^{+} = \biggl(\cos^{}{t \begin{bmatrix}
			x_1 \\
			y_1
		\end{bmatrix}} - \sin^{}{t \begin{bmatrix}
			y_1 \\
			-x_1
		\end{bmatrix}}, \sin^{}{t \begin{bmatrix}
			x_1 \\
			y_1
		\end{bmatrix}} + \cos^{}{t \begin{bmatrix}
			y_1 \\
			-x_1
		\end{bmatrix}}\biggr),
\]
donde segue que tanto a primeira quanto a segunda coordenada de \(x^{+}\) são soluções de
\[
	\begin{bmatrix}
		x \\
		y
	\end{bmatrix}^{\cdot} = A \begin{bmatrix}
		x \\
		y
	\end{bmatrix}
\]

\subsection{Espaços com Produto Interno e Espaços de Hilbert}
\begin{def*}
	Seja H um espaço vetorial sobre \(\mathbb{K}.\) Um \textbf{produto escalar} é uma função \(\left< \cdot , \cdot  \right>: H\times H\rightarrow \mathbb{R}\) tal que, para todo u, v e w de H,
	\begin{align*}
		 & (a)\; \left< u, v \right> = \overline{\left< v, u \right>}                                                                                \\
		 & (b)\; \left< \alpha u + \beta v, w \right> = \alpha \left< u, w \right> + \beta \left< v, w \right>, \quad \alpha,\; \beta \in \mathbb{K} \\
		 & (c)\; \left< u, u \right>\geq 0                                                                                                           \\
		 & (d)\; \left< u, u \right> = 0 \Longleftrightarrow u = 0.
	\end{align*}
	Um espaço vetorial H juntamente com um produto interno é dito um \textbf{espaço com produto interno.} \(\square\)
\end{def*}

Dessas propriedades, segue que, para todos u, v e w em H, e \(\alpha , \beta \) em \(\mathbb{K}\),
\[
	\left< u, \alpha v + \beta w \right> = \overline{\alpha }\left< u, v \right> + \overline{\beta }\left< u, w \right>
\]
e vale a \hypertarget{cauchy_schwarz}{desigualdade de Cauchy-Schwarz}:
\[
	| \left< u, v \right> | \leq \left< u, u \right>^{\frac{1}{2}}\left< v, v \right>^{\frac{1}{2}}.
\]
De fato, para t reais,
\[
	0\leq \left< u+tw, u+tw \right> = \left< u, u \right> + 2t \mathrm{Re}\left< u, w \right> + t^{2}\left< w, w \right>.
\]
Como a expressão do lado direito é uma função quadrática de t com uma ou nenhuma raiz real
\[
	0\geq 4(\mathrm{Re}\left< u, w \right>)^{2} - 4 \left< u, u \right>\left< w, w \right>
\]
e, se \(w = e^{i \mathrm{arg}\left< u, v \right>}v\), temos
\[
	0\geq 4 | \left< u, v \right> |^{2} - 4 \left< u, u \right>\left< v, v \right>,
\]
provando a desigualdade por manipulação algébrica:
\begin{align*}
	                    & 0\geq 4 | \left< u, v \right> |^{2} - 4 \left< u, u \right>\left< v, v \right>,      \\
	\Longleftrightarrow & 4 | \left< u, v \right> |^{2} \geq 4 \left< u, u \right>\left< v, v \right>          \\
	\Longleftrightarrow & | \left< u, v \right> | \geq (\left< u, u \right>\left< v, v \right>)^{\frac{1}{2}}.
\end{align*}

Quando temos um espaço com produto interno, ganhamos naturalmente uma norma dada por
\begin{align*}
	\Vert \cdot  \Vert: & H\rightarrow \mathbb{R}                                         \\
	                    & u\longmapsto \Vert u \Vert = \left< u, u \right>^{\frac{1}{2}}.
\end{align*}
Para ver que isto é verdade, precisamos mostrar apenas que \(\Vert u + v \Vert\leq \Vert u \Vert + \Vert v \Vert\) para todo u, v em H, que segue da \hyperlink{cauchy_schwarz}{\textit{desigualdade de Cauchy-Schwarz}} e de
\begin{align*}
	\Vert u+v \Vert^{2} & = \Vert u \Vert^{2} + 2 \mathrm{Re}\left< u, v \right> + \Vert v \Vert^{2} \\
	                    & \leq \Vert u \Vert^{2} + 2 | \left< u, v \right> | + \Vert v \Vert^{2}     \\
	                    & \leq \Vert u \Vert^{2} + 2 \Vert u \Vert\Vert v \Vert + \Vert v \Vert^{2}  \\
	                    & = (\Vert u \Vert + \Vert v \Vert)^{2}.
\end{align*}

\hypertarget{parallelogram_identity}{\begin{lemma*}[identidade do Paralelogramo]
		Em um espaço com produto interno \((H, \left< \cdot , \cdot  \right>)\), vale a identidade do paralelogramo:
		\[
			\Vert u + v \Vert^{2} + \Vert u-v \Vert^{2} = 2(\Vert u \Vert^{2} + \Vert v \Vert^{2}),\quad \forall u, v\in H.
		\]
	\end{lemma*}}
\begin{def*}
	Se um espaço com produto interno \(H\) é completo, dizemos que H é um \textbf{espaço de Hilbert.} \(\square\)
\end{def*}
\begin{def*}
	Dois vetores u e v em um espaço com produto interno H são ditos \textbf{ortogonais}, denotado por \(u\perp v\), se \(\left< u, v \right> = 0.\; \square\)
\end{def*}
\hypertarget{pythagorean_theorem}{\begin{theorem*}[Teorema de Pitágoras]
		Dados dois vetores ortogonais u e v num espaço com produto interno H, vale o Teorema de Pitágoras:
		\[
			\Vert u + v \Vert^{2} = \Vert u \Vert^{2} + \Vert v \Vert^{2}.
		\]
		Mais geralmente, se \(u_1, \dotsc , u_{n}\) são vetores dois a dois ortogonais em um espaço com produto interno H, então
		\[
			\biggl\vert \sum\limits_{i=1}^{n}u_{i} \biggr\vert^{2} = \sum\limits_{i=1}^{n}\Vert u_{i} \Vert^{2}.
		\]
	\end{theorem*}}
\begin{tcolorbox}[
		skin=enhanced,
		title=Lembrete!,
		after title={\hfill Conjuntos Convexos},
		fonttitle=\bfseries,
		sharp corners=downhill,
		colframe=black,
		colbacktitle=yellow!75!white,
		colback=yellow!30,
		colbacklower=black,
		coltitle=black,
		%drop fuzzy shadow,
		drop large lifted shadow
	]
	Lembre-se que um subconjunto C de um espaço vetorial X é dito \textbf{convexo} se, para todos x, y em C e \(t\in [0, 1]\),
	\[
		tx + (1-t)y\in C,
	\]
	ou seja, qualquer segmento de reta entre pontos de C continua inteiramente em C.
\end{tcolorbox}
\end{document}
